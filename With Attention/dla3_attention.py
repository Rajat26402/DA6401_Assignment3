# -*- coding: utf-8 -*-
"""dla3-attention.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E_zF5sEqOI6QlKkQ_JEwCCLpiiPx10XT
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

import os
import json
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import pandas as pd
from torch.utils.data import DataLoader, Dataset
import wandb

import torch
import torch.nn as nn
import torch.nn.functional as F


class AdditiveAttentionModule(nn.Module):
    """
    Implementation of additive attention mechanism (Bahdanau-style attention).

    This module computes attention weights between decoder states and encoder outputs
    using a feedforward network with tanh activation.
    """

    def __init__(self, state_dimension):
        """
        Initialize the additive attention mechanism.

        Args:
            state_dimension: Size of the hidden state vectors
        """
        super(AdditiveAttentionModule, self).__init__()
        self.state_dimension = state_dimension

        # Neural network components for attention computation
        self.alignment_network = nn.Linear(state_dimension * 2, state_dimension)
        self.attention_scorer = nn.Linear(state_dimension, 1, bias=False)

    def _compute_alignment_scores(self, decoder_state, encoder_outputs):
        """
        Compute alignment scores between decoder state and encoder outputs.

        Args:
            decoder_state: Current decoder hidden state [batch_size, state_dimension]
            encoder_outputs: All encoder hidden states [batch_size, sequence_length, state_dimension]

        Returns:
            alignment_scores: Raw alignment scores [batch_size, sequence_length]
        """
        batch_size, sequence_length, _ = encoder_outputs.size()

        # Expand decoder state to match encoder outputs dimensions
        expanded_decoder_state = decoder_state.unsqueeze(1).expand(-1, sequence_length, -1)

        # Concatenate decoder state with each encoder output
        combined_states = torch.cat((expanded_decoder_state, encoder_outputs), dim=2)

        # Apply alignment network with tanh activation
        intermediate_representation = torch.tanh(self.alignment_network(combined_states))

        # Compute final alignment scores
        raw_scores = self.attention_scorer(intermediate_representation)

        # Remove the last dimension to get [batch_size, sequence_length]
        alignment_scores = raw_scores.squeeze(2)

        return alignment_scores

    def _calculate_attention_weights(self, alignment_scores):
        """
        Convert raw alignment scores to normalized attention weights.

        Args:
            alignment_scores: Raw scores [batch_size, sequence_length]

        Returns:
            attention_weights: Normalized weights [batch_size, sequence_length]
        """
        return F.softmax(alignment_scores, dim=1)

    def _compute_context_vector(self, attention_weights, encoder_outputs):
        """
        Compute weighted context vector from encoder outputs.

        Args:
            attention_weights: Attention weights [batch_size, sequence_length]
            encoder_outputs: Encoder hidden states [batch_size, sequence_length, state_dimension]

        Returns:
            context_vector: Weighted sum of encoder outputs [batch_size, state_dimension]
        """
        # Perform batch matrix multiplication to get weighted sum
        # [batch_size, 1, sequence_length] × [batch_size, sequence_length, state_dimension]
        weighted_sum = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)

        # Remove the middle dimension to get final context vector
        context_vector = weighted_sum.squeeze(1)

        return context_vector

    def forward(self, decoder_hidden_state, encoder_sequence_outputs):
        """
        Perform forward pass of the attention mechanism.

        Args:
            decoder_hidden_state: Current decoder state [batch_size, state_dimension]
            encoder_sequence_outputs: All encoder outputs [batch_size, seq_len, state_dimension]

        Returns:
            tuple: (context_vector, attention_distribution)
                - context_vector: Weighted context [batch_size, state_dimension]
                - attention_distribution: Attention weights [batch_size, seq_len]
        """
        # Step 1: Compute raw alignment scores
        alignment_scores = self._compute_alignment_scores(
            decoder_hidden_state, encoder_sequence_outputs
        )

        # Step 2: Convert to normalized attention weights
        attention_distribution = self._calculate_attention_weights(alignment_scores)

        # Step 3: Compute weighted context vector
        context_vector = self._compute_context_vector(
            attention_distribution, encoder_sequence_outputs
        )

        return context_vector, attention_distribution

class CharacterLevelTranslationLoader(Dataset):
    """Dataset loader for character-level text translation tasks."""

    def __init__(self, data_file_path, input_vocabulary=None, output_vocabulary=None,
                 sequence_max_len=32, build_new_vocab=False):
        """
        Initialize the translation dataset.

        Args:
            data_file_path: Path to tab-separated data file
            input_vocabulary: Pre-built vocabulary for source language
            output_vocabulary: Pre-built vocabulary for target language
            sequence_max_len: Maximum sequence length for padding/truncation
            build_new_vocab: Whether to create new vocabularies from data
        """
        self.sequence_max_len = sequence_max_len
        self.translation_pairs = self._load_translation_data(data_file_path)

        # Setup vocabularies
        if build_new_vocab:
            self.input_vocabulary = self._initialize_special_tokens()
            self.output_vocabulary = self._initialize_special_tokens()
            self._construct_character_vocabularies()
        else:
            self.input_vocabulary = input_vocabulary
            self.output_vocabulary = output_vocabulary
            self._ensure_eos_tokens_exist()

    def _load_translation_data(self, file_path):
        """Load and preprocess translation pairs from file."""
        try:
            dataframe = pd.read_csv(
                file_path,
                sep='\t',
                header=None,
                names=['source_text', 'target_text', 'frequency'],
                usecols=[0, 1],
                dtype=str
            )
            print(f"Successfully loaded {len(dataframe)} translation pairs from {file_path}")

            # Handle missing values
            dataframe['source_text'] = dataframe['source_text'].fillna('')
            dataframe['target_text'] = dataframe['target_text'].fillna('')

            # Extract pairs for transliteration (roman -> native)
            pairs = list(zip(dataframe['source_text'], dataframe['target_text']))
            print(f"Sample translation pairs: {pairs[:2]}")
            return pairs

        except Exception as error:
            print(f"Error loading translation data: {error}")
            return [('', '')]  # Return empty pair as fallback

    def _initialize_special_tokens(self):
        """Create vocabulary with special tokens."""
        return {
            '<pad>': 0,    # Padding token
            '<unk>': 1,    # Unknown character token
            '<sos>': 2,    # Start of sequence token
            '<eos>': 3     # End of sequence token
        }

    def _construct_character_vocabularies(self):
        """Build character-level vocabularies from the dataset."""
        for source_text, target_text in self.translation_pairs:
            # Add source language characters
            for character in source_text:
                if character not in self.input_vocabulary:
                    self.input_vocabulary[character] = len(self.input_vocabulary)

            # Add target language characters
            for character in target_text:
                if character not in self.output_vocabulary:
                    self.output_vocabulary[character] = len(self.output_vocabulary)

        input_vocab_size = len(self.input_vocabulary)
        output_vocab_size = len(self.output_vocabulary)
        print(f"Vocabulary sizes — Input: {input_vocab_size}, Output: {output_vocab_size}")

    def _ensure_eos_tokens_exist(self):
        """Ensure end-of-sequence tokens exist in vocabularies."""
        if '<eos>' not in self.input_vocabulary:
            self.input_vocabulary['<eos>'] = len(self.input_vocabulary)
        if '<eos>' not in self.output_vocabulary:
            self.output_vocabulary['<eos>'] = len(self.output_vocabulary)

    def _text_to_indices(self, text, vocabulary):
        """Convert text to sequence of vocabulary indices."""
        indices = [vocabulary['<sos>']]  # Start with SOS token

        for char in text:
            char_index = vocabulary.get(char, vocabulary['<unk>'])
            # Validate index bounds
            if char_index >= len(vocabulary):
                char_index = vocabulary['<unk>']
            indices.append(char_index)

        indices.append(vocabulary['<eos>'])  # Add EOS token
        return indices

    def _apply_padding_and_truncation(self, sequence, vocabulary):
        """Apply padding or truncation to sequence."""
        padding_token_id = vocabulary['<pad>']

        # Calculate padding needed
        padding_length = max(0, self.sequence_max_len - len(sequence))
        padding = [padding_token_id] * padding_length

        # Apply padding and truncate if necessary
        padded_sequence = (sequence + padding)[:self.sequence_max_len]

        # Validate padding token index
        assert padding_token_id < len(vocabulary), \
            f"Padding token index {padding_token_id} exceeds vocabulary size {len(vocabulary)}"

        return padded_sequence

    def __len__(self):
        """Return the number of translation pairs in the dataset."""
        return len(self.translation_pairs)

    def __getitem__(self, index):
        """
        Get a single training example.

        Args:
            index: Index of the sample to retrieve

        Returns:
            tuple: (source_tensor, target_tensor) containing tokenized sequences
        """
        source_text, target_text = self.translation_pairs[index]

        # Convert texts to index sequences
        source_indices = self._text_to_indices(source_text, self.input_vocabulary)
        target_indices = self._text_to_indices(target_text, self.output_vocabulary)

        # Apply padding and truncation
        source_padded = self._apply_padding_and_truncation(source_indices, self.input_vocabulary)
        target_padded = self._apply_padding_and_truncation(target_indices, self.output_vocabulary)

        # Convert to PyTorch tensors
        source_tensor = torch.tensor(source_padded, dtype=torch.long)
        target_tensor = torch.tensor(target_padded, dtype=torch.long)

        return source_tensor, target_tensor

class SequenceEncoder(nn.Module):
    """
    RNN-based sequence encoder that processes input sequences and produces
    hidden representations for each time step.
    """

    def __init__(self, vocabulary_size, token_embedding_dim, state_size,
                 layer_count, regularization_rate=0.0, recurrent_unit_type='GRU'):
        """
        Initialize the sequence encoder.

        Args:
            vocabulary_size: Size of input vocabulary
            token_embedding_dim: Dimension of token embeddings
            state_size: Size of hidden states
            layer_count: Number of RNN layers
            regularization_rate: Dropout rate for regularization
            recurrent_unit_type: Type of RNN cell ('RNN', 'GRU', 'LSTM')
        """
        super(SequenceEncoder, self).__init__()
        self.token_embedder = nn.Embedding(vocabulary_size, token_embedding_dim, padding_idx=0)
        self.state_size = state_size
        self.layer_count = layer_count
        self.recurrent_unit_type = recurrent_unit_type

        # Configure the recurrent neural network
        self.sequence_processor = self._build_recurrent_network(
            token_embedding_dim, state_size, layer_count,
            regularization_rate, recurrent_unit_type
        )

    def _build_recurrent_network(self, input_dim, state_dim, num_layers,
                                dropout_rate, unit_type):
        """
        Build the appropriate RNN architecture based on specified type.

        Args:
            input_dim: Input feature dimension
            state_dim: Hidden state dimension
            num_layers: Number of stacked layers
            dropout_rate: Dropout probability
            unit_type: RNN cell type

        Returns:
            Configured RNN module
        """
        rnn_architectures = {
            'RNN': nn.RNN,
            'GRU': nn.GRU,
            'LSTM': nn.LSTM
        }

        if unit_type not in rnn_architectures:
            raise ValueError(f"Unsupported RNN architecture: {unit_type}")

        selected_rnn = rnn_architectures[unit_type]

        return selected_rnn(
            input_dim,
            state_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout_rate if num_layers > 1 else 0.0
        )

    def forward(self, input_sequence):
        """
        Process input sequence through the encoder.

        Args:
            input_sequence: Token indices [batch_size, sequence_length]

        Returns:
            tuple: (sequence_representations, final_states)
                - sequence_representations: Hidden states for all time steps
                  [batch_size, sequence_length, state_size]
                - final_states: Final hidden state [layer_count, batch_size, state_size]
        """
        # Convert token indices to dense embeddings
        token_embeddings = self.token_embedder(input_sequence)

        # Process sequence through RNN layers
        sequence_representations, final_states = self.sequence_processor(token_embeddings)

        return sequence_representations, final_states


class AttentionBasedDecoder(nn.Module):
    """
    RNN-based decoder with attention mechanism for sequence generation.

    Uses attention to dynamically focus on different parts of the encoder
    output when generating each token in the target sequence.
    """

    def __init__(self, target_vocabulary_size, token_embedding_dim, state_size,
                 layer_count, regularization_rate=0.0, recurrent_unit_type='GRU'):
        """
        Initialize the attention-based decoder.

        Args:
            target_vocabulary_size: Size of target vocabulary
            token_embedding_dim: Dimension of token embeddings
            state_size: Size of hidden states
            layer_count: Number of RNN layers
            regularization_rate: Dropout rate for regularization
            recurrent_unit_type: Type of RNN cell ('RNN', 'GRU', 'LSTM')
        """
        super(AttentionBasedDecoder, self).__init__()
        self.target_vocabulary_size = target_vocabulary_size
        self.token_embedding_dim = token_embedding_dim
        self.state_size = state_size
        self.layer_count = layer_count
        self.recurrent_unit_type = recurrent_unit_type

        # Token embedding layer
        self.token_embedder = nn.Embedding(target_vocabulary_size, token_embedding_dim, padding_idx=0)

        # Attention mechanism for context computation
        self.context_attention = AdditiveAttentionModule(state_size)

        # Combined input size (embedding + context vector)
        self.combined_input_size = token_embedding_dim + state_size

        # Build recurrent network
        self.sequence_generator = self._build_recurrent_network(
            self.combined_input_size, state_size, layer_count,
            regularization_rate, recurrent_unit_type
        )

        # Output projection layer (RNN output + context -> vocabulary logits)
        self.vocabulary_projector = nn.Linear(state_size * 2, target_vocabulary_size)

    def _build_recurrent_network(self, input_dim, state_dim, num_layers,
                                dropout_rate, unit_type):
        """Build the appropriate RNN architecture."""
        rnn_architectures = {
            'RNN': nn.RNN,
            'GRU': nn.GRU,
            'LSTM': nn.LSTM
        }

        if unit_type not in rnn_architectures:
            raise ValueError(f"Unsupported RNN architecture: {unit_type}")

        selected_rnn = rnn_architectures[unit_type]

        return selected_rnn(
            input_dim,
            state_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout_rate if num_layers > 1 else 0.0
        )

    def _extract_current_state(self, previous_states):
        """
        Extract the current hidden state for attention computation.

        Args:
            previous_states: Previous decoder states

        Returns:
            Current hidden state for the top layer
        """
        if self.recurrent_unit_type == 'LSTM':
            # For LSTM, extract hidden state (not cell state) from top layer
            return previous_states[0][-1]
        else:
            # For RNN/GRU, use hidden state from top layer
            return previous_states[-1]

    def _prepare_decoder_input(self, token_embedding, context_vector):
        """
        Combine token embedding with context vector for RNN input.

        Args:
            token_embedding: Embedded input token [batch_size, 1, embedding_dim]
            context_vector: Attention context [batch_size, state_size]

        Returns:
            Combined input tensor [batch_size, 1, embedding_dim + state_size]
        """
        return torch.cat((token_embedding, context_vector.unsqueeze(1)), dim=2)

    def _compute_output_logits(self, rnn_output, context_vector):
        """
        Compute vocabulary logits from RNN output and context.

        Args:
            rnn_output: RNN output [batch_size, 1, state_size]
            context_vector: Attention context [batch_size, state_size]

        Returns:
            Vocabulary logits [batch_size, vocabulary_size]
        """
        # Combine RNN output with context for richer representation
        combined_representation = torch.cat((rnn_output.squeeze(1), context_vector), dim=1)

        # Project to vocabulary space
        return self.vocabulary_projector(combined_representation)

    def forward(self, current_token, previous_decoder_states, encoder_sequence_outputs):
        """
        Perform one decoding step with attention.

        Args:
            current_token: Input token indices [batch_size, 1]
            previous_decoder_states: Previous decoder hidden states
            encoder_sequence_outputs: Encoder outputs [batch_size, src_len, state_size]

        Returns:
            tuple: (vocabulary_logits, new_decoder_states, attention_distribution)
                - vocabulary_logits: Output probabilities [batch_size, vocabulary_size]
                - new_decoder_states: Updated decoder states
                - attention_distribution: Attention weights [batch_size, src_len]
        """
        # Step 1: Embed the current input token
        token_embedding = self.token_embedder(current_token)

        # Step 2: Extract current state for attention computation
        current_state = self._extract_current_state(previous_decoder_states)

        # Step 3: Compute attention and context vector
        context_vector, attention_distribution = self.context_attention(
            current_state, encoder_sequence_outputs
        )

        # Step 4: Prepare combined input for RNN
        decoder_input = self._prepare_decoder_input(token_embedding, context_vector)

        # Step 5: Process through RNN
        rnn_output, new_decoder_states = self.sequence_generator(decoder_input, previous_decoder_states)

        # Step 6: Compute final output logits
        vocabulary_logits = self._compute_output_logits(rnn_output, context_vector)

        return vocabulary_logits, new_decoder_states, attention_distribution

class AttentionalSequenceTransformer(nn.Module):
    """
    Neural sequence-to-sequence model with attention mechanism for transliteration tasks.

    Combines a sequence encoder and attention-based decoder to transform
    input sequences into target sequences with dynamic attention alignment.
    """

    def __init__(self, model_configuration, source_vocabulary_size, target_vocabulary_size):
        """
        Initialize the sequence-to-sequence model with attention.

        Args:
            model_configuration: Configuration object with model hyperparameters
            source_vocabulary_size: Size of source language vocabulary
            target_vocabulary_size: Size of target language vocabulary
        """
        super(AttentionalSequenceTransformer, self).__init__()

        # Validate vocabulary sizes
        self._validate_vocabulary_sizes(source_vocabulary_size, target_vocabulary_size)

        # Extract hyperparameters from configuration
        self.token_embedding_dimension = model_configuration.embed_dim
        self.state_dimension = model_configuration.hidden_dim
        self.recurrent_architecture = model_configuration.cell_type
        self.encoding_layer_count = model_configuration.num_layers
        self.decoding_layer_count = model_configuration.num_layers

        # Initialize encoder and decoder components
        self.sequence_encoder = SequenceEncoder(
            vocabulary_size=source_vocabulary_size,
            token_embedding_dim=model_configuration.embed_dim,
            state_size=model_configuration.hidden_dim,
            layer_count=model_configuration.num_layers,
            regularization_rate=model_configuration.dropout,
            recurrent_unit_type=model_configuration.cell_type
        )

        self.sequence_decoder = AttentionBasedDecoder(
            target_vocabulary_size=target_vocabulary_size,
            token_embedding_dim=model_configuration.embed_dim,
            state_size=model_configuration.hidden_dim,
            layer_count=model_configuration.num_layers,
            regularization_rate=model_configuration.dropout,
            recurrent_unit_type=model_configuration.cell_type
        )

        # Expose embedding layers for external access
        self.source_token_embedder = self.sequence_encoder.token_embedder
        self.target_token_embedder = self.sequence_decoder.token_embedder

        self._log_model_initialization()

    def _validate_vocabulary_sizes(self, source_vocab_size, target_vocab_size):
        """Validate that vocabulary sizes are positive integers."""
        assert source_vocab_size > 0, f"Invalid source vocabulary size: {source_vocab_size}"
        assert target_vocab_size > 0, f"Invalid target vocabulary size: {target_vocab_size}"

    def _log_model_initialization(self):
        """Log model configuration details."""
        print(f"Initialized AttentionalSequenceTransformer:")
        print(f"  Architecture: {self.recurrent_architecture}")
        print(f"  Encoding layers: {self.encoding_layer_count}")
        print(f"  Decoding layers: {self.decoding_layer_count}")
        print(f"  Embedding dimension: {self.token_embedding_dimension}")
        print(f"  Hidden state dimension: {self.state_dimension}")

    def _validate_input_indices(self, source_sequence, target_sequence):
        """
        Validate and clamp input indices to prevent out-of-bounds errors.

        Args:
            source_sequence: Source token indices
            target_sequence: Target token indices

        Returns:
            tuple: (clamped_source, clamped_target)
        """
        clamped_source = source_sequence
        clamped_target = target_sequence

        # Check and clamp source indices
        max_source_index = self.source_token_embedder.num_embeddings - 1
        if source_sequence.max() >= self.source_token_embedder.num_embeddings:
            print("Warning: Source token indices exceed vocabulary bounds, clamping...")
            clamped_source = torch.clamp(source_sequence, 0, max_source_index)

        # Check and clamp target indices
        max_target_index = self.target_token_embedder.num_embeddings - 1
        if target_sequence.max() >= self.target_token_embedder.num_embeddings:
            print("Warning: Target token indices exceed vocabulary bounds, clamping...")
            clamped_target = torch.clamp(target_sequence, 0, max_target_index)

        return clamped_source, clamped_target

    def _perform_teacher_forced_decoding(self, encoder_representations, target_sequence):
        """
        Perform decoding with teacher forcing for training.

        Args:
            encoder_representations: Encoder output representations
            target_sequence: Ground truth target sequence

        Returns:
            Decoder output logits for each time step
        """
        batch_size, target_length = target_sequence.size()
        device = target_sequence.device

        # Initialize decoder state with encoder final state
        decoder_state = encoder_representations[1]  # Final encoder state

        # Prepare output tensor
        vocabulary_size = self.sequence_decoder.target_vocabulary_size
        prediction_logits = torch.zeros(batch_size, target_length-1, vocabulary_size, device=device)

        # Decode step by step with teacher forcing
        for time_step in range(target_length - 1):
            # Current input token
            current_token = target_sequence[:, time_step].unsqueeze(1)

            # Forward through decoder
            step_output, decoder_state, _ = self.sequence_decoder(
                current_token=current_token,
                previous_decoder_states=decoder_state,
                encoder_sequence_outputs=encoder_representations[0]
            )

            # Store predictions
            prediction_logits[:, time_step, :] = step_output

        return prediction_logits

    def forward(self, source_sequence, target_sequence):
        """
        Forward pass through the sequence-to-sequence model.

        Args:
            source_sequence: Source token indices [batch_size, source_length]
            target_sequence: Target token indices [batch_size, target_length]

        Returns:
            prediction_logits: Output logits [batch_size, target_length-1, vocabulary_size]
        """
        batch_size, target_length = target_sequence.size()
        device = source_sequence.device

        # Validate and clamp input indices
        validated_source, validated_target = self._validate_input_indices(
            source_sequence, target_sequence
        )

        try:
            # Encode source sequence
            encoder_representations = self.sequence_encoder(validated_source)

            # Decode with teacher forcing
            prediction_logits = self._perform_teacher_forced_decoding(
                encoder_representations, validated_target
            )

            return prediction_logits

        except Exception as error:
            print(f"Forward pass failed with error: {error}")
            vocabulary_size = self.sequence_decoder.target_vocabulary_size
            fallback_output = torch.zeros(batch_size, target_length-1, vocabulary_size, device=device)
            return fallback_output

    def _perform_greedy_generation(self, encoder_representations, maximum_sequence_length, device):
        """
        Generate sequence using greedy decoding strategy.

        Args:
            encoder_representations: Encoder output representations
            maximum_sequence_length: Maximum length for generation
            device: Target device for tensors

        Returns:
            tuple: (generated_sequences, attention_weights)
        """
        batch_size = encoder_representations[0].size(0)
        source_length = encoder_representations[0].size(1)

        # Initialize decoder state and input
        decoder_state = encoder_representations[1]
        SOS_TOKEN_ID = 2  # Start-of-sequence token
        current_input = torch.tensor([[SOS_TOKEN_ID]], device=device).repeat(batch_size, 1)

        # Storage for generated tokens and attention weights
        generated_sequence = torch.zeros(batch_size, maximum_sequence_length, dtype=torch.long, device=device)
        attention_history = torch.zeros(batch_size, maximum_sequence_length, source_length, device=device)

        # Generate tokens step by step
        for generation_step in range(maximum_sequence_length):
            # Decode current step
            step_logits, decoder_state, step_attention = self.sequence_decoder(
                current_token=current_input,
                previous_decoder_states=decoder_state,
                encoder_sequence_outputs=encoder_representations[0]
            )

            # Select most probable token (greedy strategy)
            _, most_probable_token = step_logits.topk(1)
            current_input = most_probable_token.view(batch_size, 1)

            # Store generated token and attention weights
            generated_sequence[:, generation_step] = current_input.squeeze(1)
            attention_history[:, generation_step, :] = step_attention

            # Check for early stopping (all sequences generated EOS)
            EOS_TOKEN_ID = 3  # End-of-sequence token
            if (current_input == EOS_TOKEN_ID).all():
                break

        return generated_sequence, attention_history

    def generate_sequence(self, source_sequence, maximum_length=50, search_strategy_width=1):
        """
        Generate target sequence from source sequence using specified decoding strategy.

        Args:
            source_sequence: Source token indices [batch_size, source_length]
            maximum_length: Maximum length for generated sequences
            search_strategy_width: Beam width for search (1 = greedy, >1 = beam search)

        Returns:
            tuple: (generated_sequences, attention_weights)
                - generated_sequences: Generated token sequences [batch_size, max_length]
                - attention_weights: Attention weights [batch_size, max_length, source_length]
        """
        batch_size = source_sequence.size(0)
        device = source_sequence.device

        # Encode source sequence
        encoder_representations = self.sequence_encoder(source_sequence)

        if search_strategy_width == 1:
            # Use greedy decoding
            return self._perform_greedy_generation(
                encoder_representations, maximum_length, device
            )
        else:
            # Beam search implementation placeholder
            # For now, fall back to greedy search
            print(f"Beam search with width {search_strategy_width} not yet implemented, using greedy search")
            return self._perform_greedy_generation(
                encoder_representations, maximum_length, device
            )

def compute_accuracy_function(logits, target, pad_idx=0):
    """
    Computes accuracy excluding padding tokens.
    """
    preds = logits.argmax(dim=-1)
    mask = target != pad_idx
    correct = (preds == target) & mask
    acc = correct.sum().item() / max(mask.sum().item(), 1)  # Avoid division by zero
    return acc

import torch


def execute_training_epoch(neural_network, batch_iterator, loss_criterion,
                          parameter_optimizer, computation_device):
    """
    Execute a complete training epoch over the dataset.

    Processes all batches in the data loader, computing losses, updating gradients,
    and tracking performance metrics throughout the epoch.

    Args:
        neural_network: The sequence-to-sequence model to train
        batch_iterator: DataLoader providing training batches
        loss_criterion: Loss function for computing training loss
        parameter_optimizer: Optimizer for updating model parameters
        computation_device: Device (CPU/GPU) for tensor computations

    Returns:
        tuple: (average_loss, average_accuracy)
            - average_loss: Mean loss across all processed batches
            - average_accuracy: Mean accuracy across all processed batches
    """
    # Set model to training mode
    neural_network.train()

    # Initialize performance tracking variables
    accumulated_loss = 0.0
    accumulated_accuracy = 0.0
    total_available_batches = len(batch_iterator)
    successfully_processed_batches = 0

    # Process each batch in the dataset
    for batch_index, (source_sequences, target_sequences) in enumerate(batch_iterator):

        # Attempt to process current batch
        processing_successful = _process_single_batch(
            neural_network=neural_network,
            source_data=source_sequences,
            target_data=target_sequences,
            loss_function=loss_criterion,
            optimizer=parameter_optimizer,
            device=computation_device,
            batch_number=batch_index,
            total_batches=total_available_batches
        )

        if processing_successful:
            batch_loss, batch_accuracy = processing_successful
            accumulated_loss += batch_loss
            accumulated_accuracy += batch_accuracy
            successfully_processed_batches += 1

    # Calculate epoch metrics
    return _compute_epoch_metrics(
        total_loss=accumulated_loss,
        total_accuracy=accumulated_accuracy,
        processed_count=successfully_processed_batches
    )


def _validate_batch_indices(source_batch, target_batch, model, batch_id, total_batches):
    """
    Validate that all token indices in the batch are within vocabulary bounds.

    Args:
        source_batch: Source sequence tensor
        target_batch: Target sequence tensor
        model: The neural network model
        batch_id: Current batch identifier
        total_batches: Total number of batches

    Returns:
        bool: True if batch is valid, False if should be skipped
    """
    source_max_index = source_batch.max().item()
    target_max_index = target_batch.max().item()

    source_vocabulary_limit = model.source_token_embedder.num_embeddings
    target_vocabulary_limit = model.target_token_embedder.num_embeddings

    # Check for out-of-vocabulary indices
    if (source_max_index >= source_vocabulary_limit or
        target_max_index >= target_vocabulary_limit):

        print(f"Skipping invalid batch {batch_id}/{total_batches}")
        print(f"  Source max index: {source_max_index} (limit: {source_vocabulary_limit})")
        print(f"  Target max index: {target_max_index} (limit: {target_vocabulary_limit})")
        return False

    return True


def _perform_forward_and_backward_pass(model, source_data, target_data,
                                     loss_function, optimizer):
    """
    Execute forward pass, loss computation, and backward pass for one batch.

    Args:
        model: Neural network model
        source_data: Input sequences
        target_data: Target sequences
        loss_function: Loss criterion
        optimizer: Parameter optimizer

    Returns:
        tuple: (computed_loss, model_predictions)
    """
    # Clear accumulated gradients
    optimizer.zero_grad()

    # Forward pass through the model
    prediction_logits = model(source_data, target_data)

    # Prepare data for loss computation
    flattened_predictions = prediction_logits.reshape(-1, prediction_logits.size(-1))
    shifted_targets = target_data[:, 1:].reshape(-1)  # Teacher forcing target shift

    # Compute loss
    batch_loss = loss_function(flattened_predictions, shifted_targets)

    # Backward pass
    batch_loss.backward()

    # Apply gradient clipping to prevent exploding gradients
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

    # Update model parameters
    optimizer.step()

    return batch_loss.item(), prediction_logits


def _process_single_batch(neural_network, source_data, target_data, loss_function,
                         optimizer, device, batch_number, total_batches):
    """
    Process a single training batch with error handling and validation.

    Args:
        neural_network: Model to train
        source_data: Input sequence batch
        target_data: Target sequence batch
        loss_function: Loss computation function
        optimizer: Parameter update optimizer
        device: Computation device
        batch_number: Current batch index
        total_batches: Total number of batches

    Returns:
        tuple or None: (loss, accuracy) if successful, None if failed
    """
    try:
        # Move data to appropriate device
        device_source_data = source_data.to(device)
        device_target_data = target_data.to(device)

        # Validate batch indices
        if not _validate_batch_indices(
            device_source_data, device_target_data,
            neural_network, batch_number, total_batches
        ):
            return None

        # Perform training step
        batch_loss, model_predictions = _perform_forward_and_backward_pass(
            neural_network, device_source_data, device_target_data,
            loss_function, optimizer
        )

        # Compute batch accuracy
        batch_accuracy = compute_accuracy_function(
            model_predictions, device_target_data[:, 1:]
        )

        return batch_loss, batch_accuracy

    except Exception as processing_error:
        print(f"Error processing batch {batch_number}/{total_batches}: {processing_error}")
        return None


def _compute_epoch_metrics(total_loss, total_accuracy, processed_count):
    """
    Compute final epoch metrics from accumulated values.

    Args:
        total_loss: Accumulated loss across all batches
        total_accuracy: Accumulated accuracy across all batches
        processed_count: Number of successfully processed batches

    Returns:
        tuple: (average_loss, average_accuracy)
    """
    if processed_count == 0:
        print("Warning: No batches were successfully processed in this epoch")
        return 0.0, 0.0

    epoch_average_loss = total_loss / processed_count
    epoch_average_accuracy = total_accuracy / processed_count

    return epoch_average_loss, epoch_average_accuracy

def validate_model1(model, data_loader, loss_fn, device):
    model.eval()
    total_loss = 0.0
    total_acc = 0.0
    processed_batches = 0

    with torch.no_grad():
        for inputs, targets in data_loader:
            try:
                inputs = inputs.to(device)
                targets = targets.to(device)

                # Ensure token indices are within valid vocabulary range
                if inputs.max() >= model.input_embedding.num_embeddings or \
                   targets.max() >= model.output_embedding.num_embeddings:
                    continue  # Skip batch if invalid indices found

                # Forward propagation
                predictions = model(inputs, targets)

                # Flatten tensors for loss calculation
                pred_flat = predictions.reshape(-1, predictions.size(-1))
                target_flat = targets[:, 1:].reshape(-1)  # Shift targets by one for teacher forcing

                # Compute batch loss
                loss = loss_fn(pred_flat, target_flat)
                total_loss += loss.item()

                # Compute accuracy
                acc = compute_accuracy_function(predictions, targets[:, 1:])
                total_acc += acc
                processed_batches += 1

            except Exception as ex:
                print(f"Evaluation error: {ex}")
                continue

    if processed_batches == 0:
        return 0.0, 0.0
    return total_loss / processed_batches, total_acc / processed_batches

# Dataset paths
path_train = '/kaggle/input/dakshina/dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.train.tsv'
path_dev = '/kaggle/input/dakshina/dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.dev.tsv'
vocab_dir = '/kaggle/working/vocab'

print("Generating vocabularies...")
train_data = TransliterationDataset(path_train, create_vocab=True)
src_vocab, tgt_vocab = train_data.source_vocab, train_data.target_vocab

export_vocabularies_function(vocab_dir, src_vocab, tgt_vocab)
print(f"Source vocab: {len(src_vocab)}, Target vocab: {len(tgt_vocab)}")

# Display sample vocab entries
print("Source vocab sample:")
for ch, idx in list(src_vocab.items())[:10]:
    print(f"  {repr(ch)} => {idx}")
print("Target vocab sample:")
for ch, idx in list(tgt_vocab.items())[:10]:
    print(f"  {repr(ch)} => {idx}")

def execute_hyperparameter_sweep():
    """Main execution function for hyperparameter sweep configuration"""
    experiment_run = wandb.init()
    hyperparams = experiment_run.config

    # Generate descriptive experiment identifier
    experiment_run.name = (f"{hyperparams.cell_type}-emb{hyperparams.embed_dim}-"
                          f"hid{hyperparams.hidden_dim}-layers{hyperparams.num_layers}-"
                          f"drop{hyperparams.dropout}-lr{hyperparams.learning_rate}-"
                          f"batch{hyperparams.batch_size}-beam{hyperparams.beam_size}")

    # Configure compute device with fallback mechanism
    compute_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    training_epochs = 10

    # Load or create vocabulary mappings
    try:
        source_vocabulary, target_vocabulary = load_vocabulary_mappings(vocab_dir)
        print(f"Successfully loaded vocabularies - Source: {len(source_vocabulary)}, "
              f"Target: {len(target_vocabulary)}")
    except Exception as vocab_error:
        print(f"Vocabulary loading failed: {vocab_error}")
        print("Creating vocabularies from training data...")
        training_dataset = TransliterationDataset(path_train, create_vocab=True)
        source_vocabulary = training_dataset.source_vocab
        target_vocabulary = training_dataset.target_vocab
        save_vocabulary_mappings(vocab_dir, source_vocabulary, target_vocabulary)

    # Initialize neural network model
    try:
        # Create model on CPU first for stability
        translation_model = Seq2SeqWithAttention(hyperparams, len(source_vocabulary),
                                                len(target_vocabulary))
        print("Model instantiated on CPU, transferring to target device...")
        # Transfer to target compute device
        translation_model = translation_model.to(compute_device)
        print("Model successfully transferred to compute device.")
    except Exception as model_error:
        print(f"Model initialization failed on {compute_device}: {model_error}")
        print("Reverting to CPU computation")
        compute_device = torch.device('cpu')
        translation_model = Seq2SeqWithAttention(hyperparams, len(source_vocabulary),
                                                len(target_vocabulary)).to(compute_device)

    # Prepare training and validation datasets
    try:
        print("Preparing datasets for training...")
        training_data = TransliterationDataset(path_train, source_vocabulary, target_vocabulary)
        validation_data = TransliterationDataset(path_dev, source_vocabulary, target_vocabulary)

        # Configure data loading pipelines
        train_data_loader = DataLoader(training_data, batch_size=hyperparams.batch_size,
                                     shuffle=True)
        validation_data_loader = DataLoader(validation_data, batch_size=hyperparams.batch_size)
    except Exception as data_error:
        print(f"Dataset preparation failed: {data_error}")
        return

    # Setup training components
    try:
        loss_function = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding tokens
        parameter_optimizer = optim.Adam(translation_model.parameters(),
                                       lr=hyperparams.learning_rate)

        # Configure adaptive learning rate scheduler
        lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            parameter_optimizer, mode='min', factor=0.5, patience=2, verbose=True
        )
    except Exception as setup_error:
        print(f"Training component setup failed: {setup_error}")
        return

    # Execute training procedure
    highest_validation_accuracy = 0.0

    try:
        for current_epoch in range(training_epochs):
            print(f"Training Epoch {current_epoch+1}/{training_epochs}")

            # Execute training phase
            epoch_train_loss, epoch_train_accuracy = execute_training_epoch(
                translation_model, train_data_loader, loss_function,
                parameter_optimizer, compute_device
            )
            print(f"Training metrics - Loss: {epoch_train_loss:.4f}, "
                  f"Accuracy: {epoch_train_accuracy:.4f}")

            # Execute validation phase
            epoch_val_loss, epoch_val_accuracy = evaluate_model_performance(
                translation_model, validation_data_loader, loss_function, compute_device
            )
            print(f"Validation metrics - Loss: {epoch_val_loss:.4f}, "
                  f"Accuracy: {epoch_val_accuracy:.4f}")

            # Adjust learning rate based on validation performance
            lr_scheduler.step(epoch_val_loss)

            # Record experiment metrics
            wandb.log({
                'training_loss': epoch_train_loss,
                'validation_loss': epoch_val_loss,
                'training_accuracy': epoch_train_accuracy,
                'validation_accuracy': epoch_val_accuracy,
                'epoch_number': current_epoch
            })

            # Track and save best performing model
            if epoch_val_accuracy > highest_validation_accuracy:
                highest_validation_accuracy = epoch_val_accuracy
                print(f"New optimal model achieved - Validation accuracy: "
                      f"{highest_validation_accuracy:.4f}")

        # Log final best performance metric
        wandb.log({'best_validation_accuracy': highest_validation_accuracy})


    except Exception as training_error:
        print(f"Training execution failed: {training_error}")
        import traceback
        traceback.print_exc()

# Configure sweep
sweep_config = {
    'method': 'bayes',
    'metric': {'name': 'val_accuracy', 'goal': 'maximize'},
    'parameters': {
        'cell_type': {'values': ['RNN', 'LSTM', 'GRU']},
        'embed_dim': {'values': [64, 128, 256]},
        'hidden_dim': {'values': [256, 512]},
        'num_layers': {'values': [1, 2, 3]},
        'dropout': {'values': [0.0, 0.1, 0.2]},
        'learning_rate': {'values': [0.001, 0.0005, 0.0001]},
        'batch_size': {'values': [32, 64, 128]},
        'beam_size': {'values': [1, 3, 5]}
    }
}

wandb.login(key = "d6f8c99f1fd73267470842bbf00f03ae845f7308")
sweep_id = wandb.sweep(sweep_config, project="DLA3")
wandb.agent(sweep_id, run_sweep_function, count=20)

path_test = "/kaggle/input/dakshina/dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.test.tsv"

test_dataset = TransliterationDataset(path_test, src_vocab, tgt_vocab)
test_loader = DataLoader(test_dataset, batch_size=1)

font_path = "/kaggle/input/marathi-font/AMS Aakash Regular.ttf"
marathi_font = fm.FontProperties(fname=font_path)

def convert_indices_to_text(index_sequence, vocabulary_mapping, termination_token=3):
    """
    Transform numerical index sequence into readable character string

    Args:
        index_sequence: Array of numerical indices representing tokens
        vocabulary_mapping: Dictionary containing character-to-index mappings
        termination_token: Index value indicating sequence completion (default=3)

    Returns:
        Reconstructed string from index sequence
    """
    # Build reverse lookup table (index -> character)
    index_to_character = {token_idx: character for character, token_idx in vocabulary_mapping.items()}

    # Convert indices to characters with termination handling
    character_buffer = []
    termination_encountered = False

    for token_index in index_sequence:
        if token_index == 0:  # Ignore padding tokens
            continue
        if token_index == termination_token:  # Halt at sequence terminator
            termination_encountered = True
            break
        if token_index in index_to_character:
            character_buffer.append(index_to_character[token_index])
        else:
            # Handle unknown vocabulary entries
            character_buffer.append('<UNK>')

    # Reconstruct final text output from character buffer
    # Process entire sequence if termination token wasn't found
    reconstructed_text = ''.join(character_buffer)

    return reconstructed_text

def plot_attention_heatmap_grid(selected_indices, attn_matrices, src_sequences, tgt_sequences, source_vocabulary, target_vocabulary, grid_size=9):
    """
    Renders a 3x3 grid of attention visualization heatmaps for translation examples.

    Args:
        selected_indices: List of indices to select from the dataset
        attn_matrices: Attention weight matrices for all samples
        src_sequences: Source language input sequences
        tgt_sequences: Target language output sequences
        source_vocabulary: Vocabulary mapping for source language
        target_vocabulary: Vocabulary mapping for target language
        grid_size: Number of examples to display in the grid (default: 9)

    Returns:
        figure: Matplotlib figure containing the grid of attention heatmaps
    """
    # Limit to specified grid size
    selected_indices = selected_indices[:grid_size]

    # Initialize the figure for plotting
    figure = plt.figure(figsize=(15, 15))
    grid_spec = gridspec.GridSpec(3, 3)

    for grid_pos, sample_idx in enumerate(selected_indices):
        try:
            # Extract data for the current example
            source_seq = src_sequences[sample_idx]
            target_seq = tgt_sequences[sample_idx]
            attention_map = attn_matrices[sample_idx]

            # Convert token sequences to readable text
            source_text = convert_tokens_to_text(source_seq, source_vocabulary)[5:]  # Remove <sos> token
            target_text = convert_tokens_to_text(target_seq, target_vocabulary)

            # Set up subplot
            subplot = plt.subplot(grid_spec[grid_pos])

            # Process attention weights for visualization
            attention_display = attention_map[:len(target_text), :len(source_text)].cpu().numpy()

            # Create the heatmap visualization
            heatmap = sns.heatmap(attention_display, cmap="YlGnBu", ax=subplot)

            # Configure x-axis (source text)
            source_char_count = len(source_text)
            subplot.set_xticks(np.arange(source_char_count) + 0.5)
            subplot.set_xticklabels(list(source_text))

            # Configure y-axis (target text)
            target_char_count = len(target_text)
            subplot.set_yticks(np.arange(target_char_count) + 0.5)
            subplot.set_yticklabels(list(target_text), fontproperties=marathi_font)

            # Add axis labels
            subplot.set_xlabel('Source Sequence')
            subplot.set_ylabel('Target Sequence')

            # Add title showing the translation pair
            subplot.set_title(f"Source: {source_text} → Target: {target_text}", fontproperties=marathi_font)
        except Exception as error:
            print(f"Failed to process example {sample_idx}: {error}")
            # Create placeholder for error case
            subplot = plt.subplot(grid_spec[grid_pos])
            subplot.text(0.5, 0.5, f"Error: {str(error)}", ha='center', va='center')
            subplot.axis('off')

    plt.tight_layout()
    try:
        plt.savefig("translation_attention_grid.png", dpi=300, bbox_inches="tight")
    except Exception as error:
        print(f"Warning: Failed to save visualization: {error}")
    return figure


def convert_tokens_to_text(token_sequence, vocabulary):
    """Helper function to convert token IDs to readable text"""
    # This function replaces the previous decode_sequence_function
    # Implementation would depend on your vocabulary structure
    return decode_sequence_function(token_sequence, vocabulary)

def evaluate_translation_model(model, test_dataset_loader, source_vocabulary, target_vocabulary, compute_device,
                            eos_token_id=3, unknown_token_id=1, results_path="translation_evaluation_results.csv",
                            generate_visualizations=True, visualization_sample_count=9):
    """
    Evaluates a neural translation model on test data and generates performance metrics and visualizations

    Args:
        model: The translation model to evaluate
        test_dataset_loader: DataLoader containing test examples
        source_vocabulary: Vocabulary for source language
        target_vocabulary: Vocabulary for target language
        compute_device: Device to run inference on (CPU/GPU)
        eos_token_id: ID for end-of-sequence token
        unknown_token_id: ID for unknown token (sometimes used instead of start token)
        results_path: Path to save evaluation results CSV
        generate_visualizations: Whether to create attention visualizations
        visualization_sample_count: Number of examples to include in visualizations

    Returns:
        Tuple containing: (accuracy, evaluation_results, visualization_indices,
                         attention_data, source_sequences, prediction_sequences)
    """
    model.eval()

    evaluation_results = []
    attention_data_collection = []
    source_sequences_collection = []
    target_sequences_collection = []
    prediction_sequences_collection = []

    with torch.no_grad():
        try:
            with open(results_path, 'w', newline='', encoding='utf-8') as output_file:
                csv_writer = csv.writer(output_file)
                csv_writer.writerow(['source_text', 'predicted_translation', 'reference_translation'])

                for batch_data in tqdm(test_dataset_loader, desc="Evaluating Translation"):
                    source_batch, target_batch = batch_data
                    source_batch = source_batch.to(compute_device)
                    target_batch = target_batch.to(compute_device)

                    # Generate translations with attention information
                    predictions, attention_weights = model.inference_function(source_batch)

                    # Process each example in the batch
                    for sample_idx in range(source_batch.size(0)):
                        source_tokens = source_batch[sample_idx].cpu().numpy()

                        # Handle cases where target starts with UNK token instead of SOS
                        if target_batch[sample_idx, 0].item() == unknown_token_id:
                            target_tokens = target_batch[sample_idx, 1:].cpu().numpy()  # Skip UNK token
                        else:
                            target_tokens = target_batch[sample_idx].cpu().numpy()

                        prediction_tokens = predictions[sample_idx].cpu().numpy()
                        sample_attention = attention_weights[sample_idx].cpu()

                        # Convert token sequences to readable text
                        source_text = convert_tokens_to_text(source_tokens, source_vocabulary, eos_token_id)
                        target_text = convert_tokens_to_text(target_tokens, target_vocabulary, eos_token_id)
                        prediction_text = convert_tokens_to_text(prediction_tokens, target_vocabulary, eos_token_id)

                        # Remove start token from texts
                        clean_source_text = source_text[5:]
                        clean_target_text = target_text[5:]

                        # Check if prediction matches reference
                        is_correct = prediction_text == clean_target_text

                        # Write results to CSV
                        csv_writer.writerow([
                            clean_source_text,
                            prediction_text,
                            clean_target_text
                        ])

                        # Store evaluation result
                        evaluation_results.append({
                            'source_text': clean_source_text,
                            'prediction': prediction_text,
                            'reference': clean_target_text,
                            'is_correct': is_correct
                        })

                        # Store data for visualization
                        attention_data_collection.append(sample_attention)
                        source_sequences_collection.append(source_tokens)
                        target_sequences_collection.append(target_tokens)
                        prediction_sequences_collection.append(prediction_tokens)
        except Exception as error:
            print(f"Error during evaluation output: {error}")

    # Calculate model accuracy
    correct_count = sum(1 for result in evaluation_results if result['is_correct'])
    total_count = len(evaluation_results)
    accuracy = correct_count / total_count if total_count > 0 else 0
    print(f"Translation Accuracy: {accuracy:.4f}")

    # Select representative examples for visualization
    visualization_indices = select_visualization_samples(
        evaluation_results,
        visualization_sample_count
    )

    # Generate visualizations if requested
    if generate_visualizations and visualization_indices:
        print(f"Generating visualizations for {len(visualization_indices)} examples...")

        try:
            # Create attention heatmap visualization
            plot_attention_heatmap_grid(
                visualization_indices,
                attention_data_collection,
                source_sequences_collection,
                prediction_sequences_collection,
                source_vocabulary,
                target_vocabulary
            )

            # Create connectivity visualization
            create_connectivity_visualization_grid(
                visualization_indices,
                attention_data_collection,
                source_sequences_collection,
                prediction_sequences_collection,
                source_vocabulary,
                target_vocabulary
            )

            print("Visualization generation completed successfully.")
        except Exception as error:
            print(f"Visualization generation failed: {error}")

    return (accuracy, evaluation_results, visualization_indices,
            attention_data_collection, source_sequences_collection, prediction_sequences_collection)


def select_visualization_samples(evaluation_results, sample_count):
    """
    Selects a balanced set of correct and incorrect translation examples for visualization

    Args:
        evaluation_results: List of evaluation result dictionaries
        sample_count: Number of examples to select

    Returns:
        List of indices to use for visualization
    """
    selected_indices = []

    try:
        # Separate correct and incorrect examples
        correct_indices = [i for i, result in enumerate(evaluation_results) if result['is_correct']]
        incorrect_indices = [i for i, result in enumerate(evaluation_results) if not result['is_correct']]

        # Try to select half correct examples
        target_correct_count = sample_count // 2
        if len(correct_indices) >= target_correct_count:
            selected_indices.extend(np.random.choice(correct_indices, target_correct_count, replace=False))
        else:
            selected_indices.extend(correct_indices)  # Take all available correct examples

        # Fill remaining slots with incorrect examples
        remaining_slots = sample_count - len(selected_indices)
        if remaining_slots > 0:
            if len(incorrect_indices) >= remaining_slots:
                selected_indices.extend(np.random.choice(incorrect_indices, remaining_slots, replace=False))
            else:
                # Add all available incorrect examples
                selected_indices.extend(incorrect_indices)

                # If we still need more, add additional correct examples
                additional_needed = sample_count - len(selected_indices)
                if additional_needed > 0 and len(correct_indices) > target_correct_count:
                    # Find correct examples not already selected
                    remaining_correct = [idx for idx in correct_indices if idx not in selected_indices]
                    if remaining_correct:
                        additional_correct = np.random.choice(
                            remaining_correct,
                            min(additional_needed, len(remaining_correct)),
                            replace=False
                        )
                        selected_indices.extend(additional_correct)
    except Exception as error:
        print(f"Error selecting visualization samples: {error}")
        # Fallback: use first N samples if selection logic fails
        selected_indices = list(range(min(sample_count, len(evaluation_results))))

    return selected_indices


def convert_tokens_to_text(token_sequence, vocabulary, eos_token_id):
    """Helper function to convert token IDs to readable text"""
    # Implementation depends on existing decode function
    return decode_sequence_function(token_sequence, vocabulary, eos_token_id)

def visualize_errors_function(results, n_samples=10):
    """
    Visualize character-level errors for random samples
    """
    # Sample random results (prioritize some incorrect ones)
    incorrect_samples = [r for r in results if not r['correct']]
    correct_samples = [r for r in results if r['correct']]

    if len(incorrect_samples) > 0 and len(correct_samples) > 0:
        # Try to get a mix of correct and incorrect samples
        n_incorrect = min(n_samples // 2, len(incorrect_samples))
        n_correct = n_samples - n_incorrect

        samples = (random.sample(incorrect_samples, n_incorrect) +
                  random.sample(correct_samples, min(n_correct, len(correct_samples))))
    else:
        # If all samples are correct or incorrect, just sample randomly
        samples = random.sample(results, min(n_samples, len(results)))

    html_output = "<h2>Character-level Error Visualization</h2>"
    html_output += "<p>Green: Correct characters, Red: Incorrect characters</p>"

    for i, sample in enumerate(samples):
        html_output += f"<h3>Sample {i+1}</h3>"
        html_output += generate_char_comparison_html_function(
            sample['input'],
            sample['prediction'],
            sample['target']
        )
        html_output += "<hr>"

    return HTML(html_output)

def generate_char_comparison_html_function(input_str, pred_str, target_str):
    """
    Generate HTML with character-by-character comparison
    """
    html = f"<p><b>Input:</b> {input_str}</p>"

    html += "<p><b>Prediction vs Target:</b> "

    # Extend the shorter string with spaces to match the longer one
    max_len = max(len(pred_str), len(target_str))
    pred_str_padded = pred_str.ljust(max_len)
    target_str_padded = target_str.ljust(max_len)

    for p_char, t_char in zip(pred_str_padded, target_str_padded):
        if p_char == t_char:
            html += f'<span style="color:green">{p_char}</span>'
        else:
            html += f'<span style="color:red">{p_char}</span>'

    html += "</p>"

    html += f"<p><b>Target:</b> {target_str}</p>"

    return html

api = wandb.Api()
sweep = api.sweep("da24m014-iit-madras/DLA3/sweeps/z8hzeqkz")
best_run = sweep.best_run()
best_run_config = best_run.config

best_run_config

def test_seq2seq_model(config=None):
    # Initialize wandb run
    run = wandb.init(config=config, project="DLA3")
    cfg = run.config
    epochs = 5
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    run.name = "Testing with Attention"

    # Model initialization
    try:
        # Create model on CPU first
        model = Seq2SeqWithAttention(cfg, len(src_vocab), len(tgt_vocab))
        print("Model created on CPU, trying to move to device...")

        # Move to target device
        model = model.to(device)
        print("Model successfully moved to device.")
    except Exception as e:
        print(f"Error initializing model: {e}")
        return

    # Dataset preparation
    try:
        print("Loading datasets...")
        train_dataset = TransliterationDataset(path_train, src_vocab, tgt_vocab)
        dev_dataset = TransliterationDataset(path_dev, src_vocab, tgt_vocab)

        # Create data loaders
        train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True)
        dev_loader = DataLoader(dev_dataset, batch_size=cfg.batch_size)
    except Exception as e:
        print(f"Error loading datasets: {e}")
        return

    # Training setup
    criterion = nn.CrossEntropyLoss(ignore_index=0)  # 0 is padding index
    optimizer = optim.Adam(model.parameters(), lr=cfg.learning_rate)
    best_val_acc = 0.0

    # Training loop
    for epoch in range(epochs):
        print(f"Epoch {epoch+1}/{epochs}")

        # Training phase
        train_loss, train_acc = run_epoch_function(model, train_loader, criterion, optimizer, device)
        print(f"Train loss: {train_loss:.4f} Train Accuracy: {train_acc:.4f}")

        # Validation phase
        val_loss, val_acc = validate_model1(model, dev_loader, criterion, device)
        print(f"Validation loss: {val_loss:.4f} Val Accuracy: {val_acc:.4f}")

        # Logging and saving best model
        wandb.log({
            'train_loss': train_loss,
            'val_loss': val_loss,
            'train_acc': train_acc,
            'val_acc': val_acc,
            'epoch': epoch
        })

        if best_val_acc < val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), 'model.pth')

    # Final evaluation and visualization
    try:
        # Run full validation
        test_acc, results, samples_to_visualize, all_attention_weights, all_input_seqs, all_predicted_seqs = validate_model(
            model, test_loader, src_vocab, tgt_vocab, device
        )

        # Log test accuracy
        wandb.log({'Test_acc': test_acc})

        # Log visualizations to WandB
        try:
            # Check if visualization files exist and log them
            if os.path.exists("attention_heatmaps_grid.png"):
                wandb.log({"attention_heatmaps": wandb.Image("attention_heatmaps_grid.png")})

            if os.path.exists("connectivity_visualizations_grid.png"):
                wandb.log({"connectivity_visualizations": wandb.Image("connectivity_visualizations_grid.png")})
        except Exception as e:
            print(f"Error logging visualizations to WandB: {e}")

        # Display error visualizations
        display(visualize_errors_function(results, n_samples=10))

    except Exception as e:
        print(f"Error during model validation: {e}")

test_seq2seq_model(best_run_config)

