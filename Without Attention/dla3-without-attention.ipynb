{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T12:52:30.107022Z",
     "iopub.status.busy": "2025-05-20T12:52:30.106485Z",
     "iopub.status.idle": "2025-05-20T12:52:30.112580Z",
     "shell.execute_reply": "2025-05-20T12:52:30.111813Z",
     "shell.execute_reply.started": "2025-05-20T12:52:30.106996Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T12:52:32.104730Z",
     "iopub.status.busy": "2025-05-20T12:52:32.104137Z",
     "iopub.status.idle": "2025-05-20T12:52:32.117314Z",
     "shell.execute_reply": "2025-05-20T12:52:32.116788Z",
     "shell.execute_reply.started": "2025-05-20T12:52:32.104708Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TextConversionCorpus(Dataset):\n",
    "    def __init__(self, data_file, source_dictionary=None, target_dictionary=None, sequence_limit=32, build_dictionary=False):\n",
    "        # Try to load the data file with romanized and native text\n",
    "        try:\n",
    "            df = pd.read_csv(data_file, sep='\\t', header=None,\n",
    "                          names=['original', 'transliterated', 'frequency'],\n",
    "                          usecols=[0, 1], dtype=str)\n",
    "            print(f\"Successfully loaded {len(df)} entries from {data_file}\")\n",
    "\n",
    "            # Handle missing values\n",
    "            df['original'] = df['original'].fillna('')\n",
    "            df['transliterated'] = df['transliterated'].fillna('')\n",
    "\n",
    "            # Create pairs for training\n",
    "            self.entry_pairs = list(zip(df['transliterated'], df['original']))\n",
    "            print(f\"Sample entries: {self.entry_pairs[:2]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Data loading error: {e}\")\n",
    "            self.entry_pairs = [('', '')]  # Default empty entry\n",
    "            \n",
    "        self.sequence_limit = sequence_limit\n",
    "        \n",
    "        # Setup dictionaries for conversion\n",
    "        if build_dictionary:\n",
    "            self.source_dictionary = {'<pad>': 0, '<unk>': 1, '<bos>': 2, '<eos>': 3}\n",
    "            self.target_dictionary = {'<pad>': 0, '<unk>': 1, '<bos>': 2, '<eos>': 3}\n",
    "            self._build_dictionaries()\n",
    "        else:\n",
    "            self.source_dictionary, self.target_dictionary = source_dictionary, target_dictionary\n",
    "            # Ensure special tokens exist\n",
    "            if '<eos>' not in self.source_dictionary:\n",
    "                self.source_dictionary['<eos>'] = len(self.source_dictionary)\n",
    "            if '<eos>' not in self.target_dictionary:\n",
    "                self.target_dictionary['<eos>'] = len(self.target_dictionary)\n",
    "    \n",
    "    def _build_dictionaries(self):\n",
    "        # Create character-level dictionaries from the dataset\n",
    "        for source_text, target_text in self.entry_pairs:\n",
    "            for character in source_text:\n",
    "                if character not in self.source_dictionary:\n",
    "                    self.source_dictionary[character] = len(self.source_dictionary)\n",
    "            for character in target_text:\n",
    "                if character not in self.target_dictionary:\n",
    "                    self.target_dictionary[character] = len(self.target_dictionary)\n",
    "        print(f\"Dictionary sizes — Source: {len(self.source_dictionary)}, Target: {len(self.target_dictionary)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.entry_pairs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        source_text, target_text = self.entry_pairs[index]\n",
    "        \n",
    "        # Convert source text to indices\n",
    "        source_indices = [self.source_dictionary['<bos>']]  # Begin with start token\n",
    "        for char in source_text:\n",
    "            idx = self.source_dictionary.get(char, self.source_dictionary['<unk>'])\n",
    "            if idx >= len(self.source_dictionary):\n",
    "                idx = self.source_dictionary['<unk>']  # Safety check\n",
    "            source_indices.append(idx)\n",
    "        \n",
    "        # Convert target text to indices\n",
    "        target_indices = [self.target_dictionary['<bos>']]  # Begin with start token\n",
    "        for char in target_text:\n",
    "            idx = self.target_dictionary.get(char, self.target_dictionary['<unk>'])\n",
    "            if idx >= len(self.target_dictionary):\n",
    "                idx = self.target_dictionary['<unk>']  # Safety check\n",
    "            target_indices.append(idx)\n",
    "        \n",
    "        # Add end tokens\n",
    "        source_indices.append(self.source_dictionary['<eos>'])\n",
    "        target_indices.append(self.target_dictionary['<eos>'])\n",
    "        \n",
    "        # Add padding\n",
    "        src_padding = [self.source_dictionary['<pad>']] * max(0, self.sequence_limit - len(source_indices))\n",
    "        tgt_padding = [self.target_dictionary['<pad>']] * max(0, self.sequence_limit - len(target_indices))\n",
    "        \n",
    "        # Truncate if needed and convert to tensor\n",
    "        source_indices = (source_indices + src_padding)[:self.sequence_limit]\n",
    "        target_indices = (target_indices + tgt_padding)[:self.sequence_limit]\n",
    "        \n",
    "        # Validate padding index\n",
    "        assert self.source_dictionary['<pad>'] < len(self.source_dictionary), \"Source padding index out of range\"\n",
    "        assert self.target_dictionary['<pad>'] < len(self.target_dictionary), \"Target padding index out of range\"\n",
    "        \n",
    "        return torch.tensor(source_indices, dtype=torch.long), torch.tensor(target_indices, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T12:52:33.575160Z",
     "iopub.status.busy": "2025-05-20T12:52:33.574601Z",
     "iopub.status.idle": "2025-05-20T12:52:33.597744Z",
     "shell.execute_reply": "2025-05-20T12:52:33.597126Z",
     "shell.execute_reply.started": "2025-05-20T12:52:33.575140Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SourceProcessor(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, layers_count, dropout_rate=0.0, architecture_type='GRU'):\n",
    "        super().__init__()\n",
    "        self.char_embeddings = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.layers_count = layers_count\n",
    "        self.architecture_type = architecture_type\n",
    "        \n",
    "        # Select the recurrent architecture\n",
    "        if architecture_type == 'SimpleRNN':\n",
    "            rnn_class = nn.RNN\n",
    "        elif architecture_type == 'GRU':\n",
    "            rnn_class = nn.GRU\n",
    "        elif architecture_type == 'LSTM':\n",
    "            rnn_class = nn.LSTM\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported architecture: {architecture_type}\")\n",
    "        \n",
    "        self.processor = rnn_class(\n",
    "            embed_size,\n",
    "            hidden_size,\n",
    "            num_layers=layers_count,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_rate if layers_count > 1 else 0.0\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_seq):\n",
    "        \"\"\"\n",
    "        Process input sequence through the encoder\n",
    "        \"\"\"\n",
    "        # Generate embeddings\n",
    "        embedded_chars = self.char_embeddings(input_seq)\n",
    "        \n",
    "        # Process through RNN\n",
    "        full_output, final_state = self.processor(embedded_chars)\n",
    "        \n",
    "        return full_output, final_state\n",
    "\n",
    "\n",
    "class TargetGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, layers_count, dropout_rate=0.0, architecture_type='GRU'):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.layers_count = layers_count\n",
    "        self.architecture_type = architecture_type\n",
    "        \n",
    "        self.char_embeddings = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        \n",
    "        # Select the recurrent architecture\n",
    "        if architecture_type == 'SimpleRNN':\n",
    "            rnn_class = nn.RNN\n",
    "        elif architecture_type == 'GRU':\n",
    "            rnn_class = nn.GRU\n",
    "        elif architecture_type == 'LSTM':\n",
    "            rnn_class = nn.LSTM\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported architecture: {architecture_type}\")\n",
    "        \n",
    "        self.processor = rnn_class(\n",
    "            embed_size,\n",
    "            hidden_size,\n",
    "            num_layers=layers_count,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_rate if layers_count > 1 else 0.0\n",
    "        )\n",
    "        \n",
    "        # Projection to vocabulary\n",
    "        self.output_mapper = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, token_index, prev_state):\n",
    "        \"\"\"\n",
    "        Process a single decoder step\n",
    "        \"\"\"\n",
    "        # Get embeddings for the current token\n",
    "        embedded_token = self.char_embeddings(token_index)\n",
    "        \n",
    "        # Process through RNN\n",
    "        output, new_state = self.processor(embedded_token, prev_state)\n",
    "        \n",
    "        # Project output to vocabulary size\n",
    "        output_probs = self.output_mapper(output.squeeze(1))\n",
    "        \n",
    "        return output_probs, new_state\n",
    "\n",
    "\n",
    "class TransliterationSystem(nn.Module):\n",
    "    def __init__(self, params, input_vocab_size, output_vocab_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Validate vocabulary sizes\n",
    "        assert input_vocab_size > 0, f\"Invalid input vocabulary size: {input_vocab_size}\"\n",
    "        assert output_vocab_size > 0, f\"Invalid output vocabulary size: {output_vocab_size}\"\n",
    "        \n",
    "        # Store configuration parameters\n",
    "        self.embedding_dim = params.embedding_size\n",
    "        self.internal_dim = params.internal_size\n",
    "        self.rnn_type = params.rnn_architecture\n",
    "        self.encoder_depth = params.encoder_depth\n",
    "        self.decoder_depth = params.decoder_depth\n",
    "        \n",
    "        # Initialize encoder and decoder components\n",
    "        self.encoder = SourceProcessor(\n",
    "            input_vocab_size,\n",
    "            params.embedding_size,\n",
    "            params.internal_size,\n",
    "            params.encoder_depth,\n",
    "            params.dropout_prob,\n",
    "            params.rnn_architecture\n",
    "        )\n",
    "        \n",
    "        self.decoder = TargetGenerator(\n",
    "            output_vocab_size,\n",
    "            params.embedding_size,\n",
    "            params.internal_size,\n",
    "            params.decoder_depth,\n",
    "            params.dropout_prob,\n",
    "            params.rnn_architecture\n",
    "        )\n",
    "        \n",
    "        # Store embeddings for access during training/inference\n",
    "        self.source_embeddings = self.encoder.char_embeddings\n",
    "        self.target_embeddings = self.decoder.char_embeddings\n",
    "        \n",
    "        print(f\"Model initialized: {self.rnn_type}, Encoder depth: {self.encoder_depth}, \"\n",
    "              f\"Decoder depth: {self.decoder_depth}, Embeddings: {self.embedding_dim}, \"\n",
    "              f\"Hidden size: {self.internal_dim}\")\n",
    "              \n",
    "    def forward(self, source_sequence, target_sequence):\n",
    "        batch_size, src_len = source_sequence.size()\n",
    "        tgt_len = target_sequence.size(1)\n",
    "        device = source_sequence.device\n",
    "        \n",
    "        # Check for out-of-bounds indices and clamp if needed\n",
    "        if source_sequence.max() >= self.source_embeddings.num_embeddings:\n",
    "            print(\"Warning: Source indices out of vocabulary range\")\n",
    "            source_sequence = torch.clamp(source_sequence, 0, self.source_embeddings.num_embeddings - 1)\n",
    "        if target_sequence.max() >= self.target_embeddings.num_embeddings:\n",
    "            print(\"Warning: Target indices out of vocabulary range\")\n",
    "            target_sequence = torch.clamp(target_sequence, 0, self.target_embeddings.num_embeddings - 1)\n",
    "            \n",
    "        try:\n",
    "            # Process source sequence\n",
    "            _, encoder_state = self.encoder(source_sequence)\n",
    "            \n",
    "            # Handle mismatch in number of layers between encoder and decoder\n",
    "            if self.encoder_depth != self.decoder_depth:\n",
    "                # If encoder and decoder depths are different, need to adjust the hidden state\n",
    "                if isinstance(encoder_state, tuple):  # For LSTM (hidden state, cell state)\n",
    "                    # Adjust both hidden state and cell state\n",
    "                    h_state, c_state = encoder_state\n",
    "                    if self.encoder_depth > self.decoder_depth:\n",
    "                        # If encoder has more layers, take only what decoder needs\n",
    "                        decoder_h = h_state[-self.decoder_depth:]\n",
    "                        decoder_c = c_state[-self.decoder_depth:]\n",
    "                        decoder_state = (decoder_h, decoder_c)\n",
    "                    else:\n",
    "                        # If decoder has more layers, replicate encoder's last layer\n",
    "                        decoder_h = torch.cat([h_state, h_state[-1:].repeat(self.decoder_depth - self.encoder_depth, 1, 1)], 0)\n",
    "                        decoder_c = torch.cat([c_state, c_state[-1:].repeat(self.decoder_depth - self.encoder_depth, 1, 1)], 0)\n",
    "                        decoder_state = (decoder_h, decoder_c)\n",
    "                else:  # For GRU/RNN (just hidden state)\n",
    "                    if self.encoder_depth > self.decoder_depth:\n",
    "                        # If encoder has more layers, take only what decoder needs\n",
    "                        decoder_state = encoder_state[-self.decoder_depth:]\n",
    "                    else:\n",
    "                        # If decoder has more layers, replicate encoder's last layer\n",
    "                        decoder_state = torch.cat([encoder_state, \n",
    "                                                  encoder_state[-1:].repeat(self.decoder_depth - self.encoder_depth, 1, 1)], 0)\n",
    "            else:\n",
    "                # If depths match, use encoder state directly\n",
    "                decoder_state = encoder_state\n",
    "            \n",
    "            # Teacher forcing: use target tokens as inputs\n",
    "            decoder_inputs = target_sequence[:, :-1]  # From <bos> to second-to-last token\n",
    "            \n",
    "            # Create tensor for outputs\n",
    "            outputs = torch.zeros(batch_size, tgt_len-1, self.decoder.vocab_size, device=device)\n",
    "            \n",
    "            # Process each token sequentially\n",
    "            for t in range(tgt_len-1):\n",
    "                # Get current token\n",
    "                current_token = target_sequence[:, t].unsqueeze(1)\n",
    "                \n",
    "                # Process through decoder\n",
    "                decoder_output, decoder_state = self.decoder(\n",
    "                    current_token,\n",
    "                    decoder_state\n",
    "                )\n",
    "                \n",
    "                # Store prediction\n",
    "                outputs[:, t, :] = decoder_output\n",
    "                \n",
    "            return outputs\n",
    "            \n",
    "        except Exception as error:\n",
    "            print(f\"Error in forward pass: {error}\")\n",
    "            return torch.zeros(batch_size, tgt_len-1, self.decoder.vocab_size, device=device)\n",
    "            \n",
    "    def generate(self, source_sequence, max_length=50, beam_width=1):\n",
    "\n",
    "        batch_size = source_sequence.size(0)\n",
    "        device = source_sequence.device\n",
    "        \n",
    "        # Encode source sequence\n",
    "        _, encoder_state = self.encoder(source_sequence)\n",
    "        \n",
    "        # Handle mismatch in number of layers between encoder and decoder\n",
    "        if self.encoder_depth != self.decoder_depth:\n",
    "            # If encoder and decoder depths are different, need to adjust the hidden state\n",
    "            if isinstance(encoder_state, tuple):  # For LSTM (hidden state, cell state)\n",
    "                # Adjust both hidden state and cell state\n",
    "                h_state, c_state = encoder_state\n",
    "                if self.encoder_depth > self.decoder_depth:\n",
    "                    # If encoder has more layers, take only what decoder needs\n",
    "                    decoder_h = h_state[-self.decoder_depth:]\n",
    "                    decoder_c = c_state[-self.decoder_depth:]\n",
    "                    decoder_state = (decoder_h, decoder_c)\n",
    "                else:\n",
    "                    # If decoder has more layers, replicate encoder's last layer\n",
    "                    decoder_h = torch.cat([h_state, h_state[-1:].repeat(self.decoder_depth - self.encoder_depth, 1, 1)], 0)\n",
    "                    decoder_c = torch.cat([c_state, c_state[-1:].repeat(self.decoder_depth - self.encoder_depth, 1, 1)], 0)\n",
    "                    decoder_state = (decoder_h, decoder_c)\n",
    "            else:  # For GRU/RNN (just hidden state)\n",
    "                if self.encoder_depth > self.decoder_depth:\n",
    "                    # If encoder has more layers, take only what decoder needs\n",
    "                    decoder_state = encoder_state[-self.decoder_depth:]\n",
    "                else:\n",
    "                    # If decoder has more layers, replicate encoder's last layer\n",
    "                    decoder_state = torch.cat([encoder_state, \n",
    "                                              encoder_state[-1:].repeat(self.decoder_depth - self.encoder_depth, 1, 1)], 0)\n",
    "        else:\n",
    "            # If depths match, use encoder state directly\n",
    "            decoder_state = encoder_state\n",
    "        \n",
    "        if beam_width == 1:\n",
    "            # Use greedy search\n",
    "            decoder_token = torch.tensor([[2]], device=device).repeat(batch_size, 1)  # <bos> token\n",
    "            output_sequence = torch.zeros(batch_size, max_length, dtype=torch.long, device=device)\n",
    "            \n",
    "            for t in range(max_length):\n",
    "                # Process through decoder\n",
    "                token_probs, decoder_state = self.decoder(\n",
    "                    decoder_token,\n",
    "                    decoder_state\n",
    "                )\n",
    "                \n",
    "                # Select most likely token\n",
    "                _, token_idx = token_probs.topk(1)\n",
    "                decoder_token = token_idx.view(batch_size, 1)\n",
    "                \n",
    "                # Store generated token\n",
    "                output_sequence[:, t] = decoder_token.squeeze(1)\n",
    "                \n",
    "                # Check for end-of-sequence\n",
    "                if (decoder_token == 3).all():  # 3 is <eos> token\n",
    "                    break\n",
    "                    \n",
    "            return output_sequence\n",
    "        else:\n",
    "            # Placeholder for beam search implementation\n",
    "            # Currently falls back to greedy search\n",
    "            return self.generate(source_sequence, max_length, beam_width=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T12:52:40.831948Z",
     "iopub.status.busy": "2025-05-20T12:52:40.831178Z",
     "iopub.status.idle": "2025-05-20T12:52:40.840706Z",
     "shell.execute_reply": "2025-05-20T12:52:40.839939Z",
     "shell.execute_reply.started": "2025-05-20T12:52:40.831923Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def training_iteration(model, dataloader, loss_function, optimizer, compute_device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_performance = 0.0\n",
    "    total_batches = len(dataloader)\n",
    "    successful_batches = 0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "        try:\n",
    "            inputs = inputs.to(compute_device)\n",
    "            targets = targets.to(compute_device)\n",
    "            \n",
    "            # Check for vocabulary issues\n",
    "            if inputs.max().item() >= model.source_embeddings.num_embeddings or \\\n",
    "               targets.max().item() >= model.target_embeddings.num_embeddings:\n",
    "                print(f\"Skipping batch {batch_idx}/{total_batches} - vocabulary index issues detected\")\n",
    "                continue\n",
    "                \n",
    "            # Clear gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = model(inputs, targets)\n",
    "            \n",
    "            # Reshape for loss calculation\n",
    "            flat_predictions = predictions.reshape(-1, predictions.size(-1))\n",
    "            flat_targets = targets[:, 1:].reshape(-1)  # Offset for teacher forcing\n",
    "            \n",
    "            # Calculate loss and backpropagate\n",
    "            batch_loss = loss_function(flat_predictions, flat_targets)\n",
    "            batch_loss.backward()\n",
    "            \n",
    "            # Apply gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            total_loss += batch_loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            batch_accuracy = calculate_performance(predictions, targets[:, 1:])\n",
    "            total_performance += batch_accuracy\n",
    "            successful_batches += 1\n",
    "            \n",
    "        except Exception as error:\n",
    "            print(f\"Error processing batch {batch_idx}/{total_batches}: {error}\")\n",
    "            continue\n",
    "    \n",
    "    if successful_batches == 0:\n",
    "        return 0.0, 0.0\n",
    "    return total_loss / successful_batches, total_performance / successful_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T12:52:43.093161Z",
     "iopub.status.busy": "2025-05-20T12:52:43.092911Z",
     "iopub.status.idle": "2025-05-20T12:52:43.100920Z",
     "shell.execute_reply": "2025-05-20T12:52:43.100346Z",
     "shell.execute_reply.started": "2025-05-20T12:52:43.093143Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def validation_check(model, dataloader, loss_function, compute_device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_performance = 0.0\n",
    "    successful_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            try:\n",
    "                inputs = inputs.to(compute_device)\n",
    "                targets = targets.to(compute_device)\n",
    "                \n",
    "                # Check for vocabulary issues\n",
    "                if inputs.max() >= model.source_embeddings.num_embeddings or \\\n",
    "                   targets.max() >= model.target_embeddings.num_embeddings:\n",
    "                    continue\n",
    "                \n",
    "                # Forward pass\n",
    "                predictions = model(inputs, targets)\n",
    "                \n",
    "                # Reshape for loss calculation\n",
    "                flat_predictions = predictions.reshape(-1, predictions.size(-1))\n",
    "                flat_targets = targets[:, 1:].reshape(-1)  # Offset for teacher forcing\n",
    "                \n",
    "                # Calculate loss\n",
    "                batch_loss = loss_function(flat_predictions, flat_targets)\n",
    "                total_loss += batch_loss.item()\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                batch_accuracy = calculate_performance(predictions, targets[:, 1:])\n",
    "                total_performance += batch_accuracy\n",
    "                successful_batches += 1\n",
    "                \n",
    "            except Exception as error:\n",
    "                print(f\"Evaluation error: {error}\")\n",
    "                continue\n",
    "    \n",
    "    if successful_batches == 0:\n",
    "        return 0.0, 0.0\n",
    "    return total_loss / successful_batches, total_performance / successful_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T12:52:43.848221Z",
     "iopub.status.busy": "2025-05-20T12:52:43.847964Z",
     "iopub.status.idle": "2025-05-20T12:52:43.854572Z",
     "shell.execute_reply": "2025-05-20T12:52:43.853758Z",
     "shell.execute_reply.started": "2025-05-20T12:52:43.848203Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "hyperparameter_search = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {'name': 'validation_accuracy', 'goal': 'maximize'},\n",
    "    'parameters': {\n",
    "        'embedding_size': {'values': [16, 32, 64, 256]},\n",
    "        'internal_size': {'values': [16, 32, 64, 256]},\n",
    "        'rnn_architecture': {'values': ['SimpleRNN', 'GRU', 'LSTM']},\n",
    "        'encoder_depth': {'values': [1, 2, 3]},\n",
    "        'decoder_depth': {'values': [1, 2, 3]},\n",
    "        'dropout_prob': {'values': [0.2, 0.3]},\n",
    "        'learning_rate': {'values': [1e-3, 1e-4]},\n",
    "        'batch_size': {'values': [32, 64]},\n",
    "        'beam_width': {'values': [1, 3, 5]}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T12:52:46.311698Z",
     "iopub.status.busy": "2025-05-20T12:52:46.311077Z",
     "iopub.status.idle": "2025-05-20T12:52:46.486450Z",
     "shell.execute_reply": "2025-05-20T12:52:46.485750Z",
     "shell.execute_reply.started": "2025-05-20T12:52:46.311673Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_file = '/kaggle/input/dakshina/dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.train.tsv'\n",
    "dev_file = '/kaggle/input/dakshina/dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.dev.tsv'\n",
    "dict_dir = '/kaggle/working/dictionaries'\n",
    "\n",
    "print(\"Building dictionaries...\")\n",
    "train_corpus = TextConversionCorpus(train_file, build_dictionary=True)\n",
    "source_dict, target_dict = train_corpus.source_dictionary, train_corpus.target_dictionary\n",
    "\n",
    "save_dictionaries(dict_dir, source_dict, target_dict)\n",
    "print(f\"Source dictionary: {len(source_dict)}, Target dictionary: {len(target_dict)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T12:52:47.544945Z",
     "iopub.status.busy": "2025-05-20T12:52:47.544400Z",
     "iopub.status.idle": "2025-05-20T12:52:47.554735Z",
     "shell.execute_reply": "2025-05-20T12:52:47.554162Z",
     "shell.execute_reply.started": "2025-05-20T12:52:47.544922Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def execute_hyperparameter_search():\n",
    "    experiment = wandb.init()\n",
    "    config = experiment.config\n",
    "        \n",
    "    # Create descriptive experiment name\n",
    "    experiment.name = f\"{config.rnn_architecture}-e{config.embedding_size}-h{config.internal_size}-enc{config.encoder_depth}-dec{config.decoder_depth}-d{config.dropout_prob}-lr{config.learning_rate}-b{config.batch_size}-beam{config.beam_width}\"\n",
    "        \n",
    "    # Setup hardware\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    training_epochs = 20\n",
    "    \n",
    "    # Initialize the model\n",
    "    try:\n",
    "        # Create on CPU first\n",
    "        model = TransliterationSystem(config, len(source_dict), len(target_dict))\n",
    "        print(\"Model created, moving to target device...\")\n",
    "        # Move to target device\n",
    "        model = model.to(device)\n",
    "        print(f\"Model successfully loaded on {device}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing model on {device}: {e}\")\n",
    "        print(\"Falling back to CPU...\")\n",
    "        device = torch.device('cpu')\n",
    "        model = TransliterationSystem(config, len(source_dict), len(target_dict)).to(device)\n",
    "        \n",
    "    # Setup datasets\n",
    "    try:\n",
    "        print(\"Preparing datasets...\")\n",
    "        train_corpus = TextConversionCorpus(train_file, source_dict, target_dict)\n",
    "        dev_corpus = TextConversionCorpus(dev_file, source_dict, target_dict)\n",
    "            \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(train_corpus, batch_size=config.batch_size, shuffle=True)\n",
    "        dev_loader = DataLoader(dev_corpus, batch_size=config.batch_size)\n",
    "    except Exception as e:\n",
    "        print(f\"Dataset preparation error: {e}\")\n",
    "        return\n",
    "        \n",
    "    # Setup training components\n",
    "    try:\n",
    "        loss_function = nn.CrossEntropyLoss(ignore_index=0)  # 0 is padding index\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "    except Exception as e:\n",
    "        print(f\"Error setting up training components: {e}\")\n",
    "        return\n",
    "        \n",
    "    # Training loop\n",
    "    best_validation_accuracy = 0.0\n",
    "        \n",
    "    try:\n",
    "        for epoch in range(training_epochs):\n",
    "            print(f\"Epoch {epoch+1}/{training_epochs}\")\n",
    "                \n",
    "            # Train\n",
    "            train_loss, train_accuracy = training_iteration(model, train_loader, loss_function, optimizer, device)\n",
    "            print(f\"Training - Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}\")\n",
    "                \n",
    "            # Validate\n",
    "            val_loss, val_accuracy = validation_check(model, dev_loader, loss_function, device)\n",
    "            print(f\"Validation - Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}\")\n",
    "                \n",
    "            # Log metrics\n",
    "            wandb.log({\n",
    "                'training_loss': train_loss, \n",
    "                'validation_loss': val_loss, \n",
    "                'training_accuracy': train_accuracy, \n",
    "                'validation_accuracy': val_accuracy, \n",
    "                'epoch': epoch\n",
    "            })\n",
    "                \n",
    "            # Track best model\n",
    "            if val_accuracy > best_validation_accuracy:\n",
    "                best_validation_accuracy = val_accuracy\n",
    "                \n",
    "        # Log final best accuracy\n",
    "        wandb.log({'validation_accuracy': best_validation_accuracy})\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"Training error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T12:52:51.010750Z",
     "iopub.status.busy": "2025-05-20T12:52:51.010256Z",
     "iopub.status.idle": "2025-05-20T12:52:51.017791Z",
     "shell.execute_reply": "2025-05-20T12:52:51.017091Z",
     "shell.execute_reply.started": "2025-05-20T12:52:51.010726Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key = \"d6f8c99f1fd73267470842bbf00f03ae845f7308\")\n",
    "sweep_id = wandb.sweep(hyperparameter_search, project='DLA3')\n",
    "wandb.agent(sweep_id, execute_hyperparameter_search, count=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T12:52:55.925315Z",
     "iopub.status.busy": "2025-05-20T12:52:55.925063Z",
     "iopub.status.idle": "2025-05-20T12:52:55.930813Z",
     "shell.execute_reply": "2025-05-20T12:52:55.930090Z",
     "shell.execute_reply.started": "2025-05-20T12:52:55.925295Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "path_test = '/kaggle/input/dakshina/dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.test.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T12:52:57.366983Z",
     "iopub.status.busy": "2025-05-20T12:52:57.366731Z",
     "iopub.status.idle": "2025-05-20T12:52:57.385304Z",
     "shell.execute_reply": "2025-05-20T12:52:57.384745Z",
     "shell.execute_reply.started": "2025-05-20T12:52:57.366965Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 5682 entries from /kaggle/input/dakshina/dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.test.tsv\n",
      "Sample entries: [('andarasan', 'अँडरसन'), ('andarasana', 'अँडरसन')]\n"
     ]
    }
   ],
   "source": [
    "test_dataset = TextConversionCorpus(path_test, source_dict, target_dict)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T12:52:58.811790Z",
     "iopub.status.busy": "2025-05-20T12:52:58.811131Z",
     "iopub.status.idle": "2025-05-20T12:52:58.821318Z",
     "shell.execute_reply": "2025-05-20T12:52:58.820569Z",
     "shell.execute_reply.started": "2025-05-20T12:52:58.811769Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def inference_without_teacher_forcing(model, src, max_len=100, device=None):\n",
    "    \"\"\"\n",
    "    Generate sequence without teacher forcing\n",
    "    \n",
    "    \"\"\"\n",
    "    batch_size = src.size(0)\n",
    "    if device is None:\n",
    "        device = src.device\n",
    "    \n",
    "    # Initialize outputs tensor\n",
    "    outputs = torch.zeros(batch_size, max_len, dtype=torch.long, device=device)\n",
    "    \n",
    "    # Encode source sequence\n",
    "    _, encoder_state = model.encoder(src)\n",
    "    \n",
    "    # Handle mismatch in number of layers between encoder and decoder\n",
    "    if model.encoder_depth != model.decoder_depth:\n",
    "        # If encoder and decoder depths are different, need to adjust the hidden state\n",
    "        if isinstance(encoder_state, tuple):  # For LSTM (hidden state, cell state)\n",
    "            # Adjust both hidden state and cell state\n",
    "            h_state, c_state = encoder_state\n",
    "            if model.encoder_depth > model.decoder_depth:\n",
    "                # If encoder has more layers, take only what decoder needs\n",
    "                decoder_h = h_state[-model.decoder_depth:]\n",
    "                decoder_c = c_state[-model.decoder_depth:]\n",
    "                decoder_state = (decoder_h, decoder_c)\n",
    "            else:\n",
    "                # If decoder has more layers, replicate encoder's last layer\n",
    "                decoder_h = torch.cat([h_state, h_state[-1:].repeat(model.decoder_depth - model.encoder_depth, 1, 1)], 0)\n",
    "                decoder_c = torch.cat([c_state, c_state[-1:].repeat(model.decoder_depth - model.encoder_depth, 1, 1)], 0)\n",
    "                decoder_state = (decoder_h, decoder_c)\n",
    "        else:  # For GRU/RNN (just hidden state)\n",
    "            if model.encoder_depth > model.decoder_depth:\n",
    "                # If encoder has more layers, take only what decoder needs\n",
    "                decoder_state = encoder_state[-model.decoder_depth:]\n",
    "            else:\n",
    "                # If decoder has more layers, replicate encoder's last layer\n",
    "                decoder_state = torch.cat([encoder_state, \n",
    "                                          encoder_state[-1:].repeat(model.decoder_depth - model.encoder_depth, 1, 1)], 0)\n",
    "    else:\n",
    "        # If depths match, use encoder state directly\n",
    "        decoder_state = encoder_state\n",
    "    \n",
    "    # Start with <bos> token (assuming token ID 2 is BOS)\n",
    "    decoder_token = torch.full((batch_size, 1), 2, dtype=torch.long, device=device)\n",
    "    \n",
    "    for t in range(max_len):\n",
    "        # Process through decoder\n",
    "        # IMPORTANT: Ensure token_index has shape [batch_size, 1]\n",
    "        decoder_output, decoder_state = model.decoder(\n",
    "            decoder_token,\n",
    "            decoder_state\n",
    "        )\n",
    "        \n",
    "        # Get most likely token\n",
    "        _, topi = decoder_output.topk(1)\n",
    "        decoder_token = topi.view(batch_size, 1)  # Ensure shape is [batch_size, 1]\n",
    "        \n",
    "        # Save to outputs tensor\n",
    "        outputs[:, t] = decoder_token.squeeze(1)\n",
    "        \n",
    "        # Check if all sequences generated EOS token (assuming token ID 3 is EOS)\n",
    "        if (decoder_token == 3).all():\n",
    "            break\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T12:53:00.486121Z",
     "iopub.status.busy": "2025-05-20T12:53:00.485686Z",
     "iopub.status.idle": "2025-05-20T12:53:00.492343Z",
     "shell.execute_reply": "2025-05-20T12:53:00.491710Z",
     "shell.execute_reply.started": "2025-05-20T12:53:00.486099Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def decode_seq(seq, char_vocab, end_token=3):\n",
    "    idx_to_char = {idx: ch for ch, idx in char_vocab.items()}\n",
    "    result = []\n",
    "    has_end_token = False\n",
    "    for idx in seq:\n",
    "        if idx == 0:  # Skip padding\n",
    "            continue\n",
    "        if idx == end_token:  # Stop at EOS\n",
    "            has_end_token = True\n",
    "            break\n",
    "        if idx in idx_to_char:\n",
    "            result.append(idx_to_char[idx])\n",
    "        else:\n",
    "            result.append('<UNK>')\n",
    "    decoded = ''.join(result)\n",
    "    return decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T12:57:04.140616Z",
     "iopub.status.busy": "2025-05-20T12:57:04.139968Z",
     "iopub.status.idle": "2025-05-20T12:57:04.150157Z",
     "shell.execute_reply": "2025-05-20T12:57:04.149507Z",
     "shell.execute_reply.started": "2025-05-20T12:57:04.140593Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def assess_model_performance(\n",
    "    neural_model, \n",
    "    evaluation_data, \n",
    "    source_vocabulary, \n",
    "    target_vocabulary, \n",
    "    processing_device, \n",
    "    eos_token_id=3, \n",
    "    unknown_token_id=1, \n",
    "    results_path='prediction_vanilla.csv'\n",
    "):\n",
    "    # Set model to evaluation mode\n",
    "    neural_model.eval()\n",
    "    \n",
    "    # Initialize tracking variables\n",
    "    evaluation_results = []\n",
    "    target_vocab_mapping = {idx: character for character, idx in target_vocabulary.items()}\n",
    "    \n",
    "    # Log vocabulary information\n",
    "    print(f'Target vocabulary size: {len(target_vocabulary)}')\n",
    "    print(f'Special tokens: PAD={0}, UNK={unknown_token_id}, SOS={2}, EOS={eos_token_id}')\n",
    "    \n",
    "    # Process evaluation data without gradient tracking\n",
    "    with torch.no_grad():\n",
    "        # Open output file for writing results\n",
    "        with open(results_path, 'w', newline='', encoding='utf-8') as output_file:\n",
    "            csv_writer = csv.writer(output_file)\n",
    "            csv_writer.writerow(['input', 'prediction', 'target', 'correct'])\n",
    "            \n",
    "            # Process each batch in the evaluation dataset\n",
    "            for data_batch in tqdm(evaluation_data, desc='Evaluating'):\n",
    "                # Extract and move data to appropriate device\n",
    "                source_data, target_data = data_batch\n",
    "                source_data = source_data.to(processing_device)\n",
    "                target_data = target_data.to(processing_device)\n",
    "                \n",
    "                # Generate predictions\n",
    "                if hasattr(neural_model, 'inference'):\n",
    "                    predictions = neural_model.inference(source_data)\n",
    "                else:\n",
    "                    predictions = inference_without_teacher_forcing(\n",
    "                        neural_model, \n",
    "                        source_data, \n",
    "                        max_len=100, \n",
    "                        device=processing_device\n",
    "                    )\n",
    "                \n",
    "                # Process each sequence in the batch\n",
    "                for i in range(source_data.size(0)):\n",
    "                    # Convert tensors to numpy arrays for processing\n",
    "                    source_sequence = source_data[i].cpu().numpy()\n",
    "                    \n",
    "                    # Handle special case for target sequences\n",
    "                    if target_data[i, 0].item() == unknown_token_id:\n",
    "                        target_sequence = target_data[i, 1:].cpu().numpy()\n",
    "                    else:\n",
    "                        target_sequence = target_data[i].cpu().numpy()\n",
    "                    \n",
    "                    prediction_sequence = predictions[i].cpu().numpy()\n",
    "                    \n",
    "                    # Decode sequences to readable strings\n",
    "                    source_string = decode_seq(source_sequence, source_vocabulary, eos_token_id)\n",
    "                    target_string = decode_seq(target_sequence, target_vocabulary, eos_token_id)\n",
    "                    prediction_string = decode_seq(prediction_sequence, target_vocabulary, eos_token_id)\n",
    "                    \n",
    "                    # Remove prefix (first 5 characters) from input and target\n",
    "                    trimmed_source = source_string[5:]\n",
    "                    trimmed_target = target_string[5:]\n",
    "                    \n",
    "                    # Check prediction accuracy\n",
    "                    is_correct = prediction_string == trimmed_target\n",
    "                    \n",
    "                    # Write results to CSV\n",
    "                    csv_writer.writerow([trimmed_source, prediction_string, trimmed_target])\n",
    "                    \n",
    "                    # Store results for accuracy calculation\n",
    "                    evaluation_results.append({\n",
    "                        'input': trimmed_source, \n",
    "                        'prediction': prediction_string, \n",
    "                        'target': trimmed_target, \n",
    "                        'correct': is_correct\n",
    "                    })\n",
    "    \n",
    "    # Calculate overall accuracy\n",
    "    correct_count = sum(1 for result in evaluation_results if result['correct'])\n",
    "    total_count = len(evaluation_results)\n",
    "    accuracy = correct_count / total_count if total_count > 0 else 0\n",
    "    \n",
    "    print(f'Overall Word Accuracy: {accuracy:.4f}')\n",
    "    \n",
    "    return accuracy, evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T12:57:05.729831Z",
     "iopub.status.busy": "2025-05-20T12:57:05.729591Z",
     "iopub.status.idle": "2025-05-20T12:57:05.736056Z",
     "shell.execute_reply": "2025-05-20T12:57:05.735424Z",
     "shell.execute_reply.started": "2025-05-20T12:57:05.729814Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_string_diff_visualization(original_input, predicted_output, expected_output):\n",
    "    # Build HTML output with comparison visualization\n",
    "    result_html = f\"<p><b>Original:</b> {original_input}</p>\"\n",
    "    \n",
    "    # Prepare for character comparison\n",
    "    comparison_html = \"<p><b>Comparison:</b> \"\n",
    "    max_string_length = max(len(predicted_output), len(expected_output))\n",
    "    \n",
    "    # Pad strings to equal length for comparison\n",
    "    pred_padded = predicted_output + \" \" * (max_string_length - len(predicted_output))\n",
    "    expect_padded = expected_output + \" \" * (max_string_length - len(expected_output))\n",
    "    \n",
    "    # Compare characters and highlight differences\n",
    "    for idx in range(max_string_length):\n",
    "        current_pred_char = pred_padded[idx]\n",
    "        current_expect_char = expect_padded[idx]\n",
    "        \n",
    "        if current_pred_char == current_expect_char:\n",
    "            comparison_html += f'<span style=\"color:green\">{current_pred_char}</span>'\n",
    "        else:\n",
    "            comparison_html += f'<span style=\"color:red\">{current_pred_char}</span>'\n",
    "    \n",
    "    comparison_html += \"</p>\"\n",
    "    result_html += comparison_html\n",
    "    result_html += f\"<p><b>Expected:</b> {expected_output}</p>\"\n",
    "    \n",
    "    return result_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T12:57:07.058567Z",
     "iopub.status.busy": "2025-05-20T12:57:07.058293Z",
     "iopub.status.idle": "2025-05-20T12:57:07.065542Z",
     "shell.execute_reply": "2025-05-20T12:57:07.064848Z",
     "shell.execute_reply.started": "2025-05-20T12:57:07.058531Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def display_prediction_analysis(evaluation_results, sample_count=10):\n",
    "\n",
    "    # Separate results into correct and incorrect predictions\n",
    "    failed_predictions = [result for result in evaluation_results if not result['correct']]\n",
    "    successful_predictions = [result for result in evaluation_results if result['correct']]\n",
    "    \n",
    "    # Select a balanced sample of results for visualization\n",
    "    if failed_predictions and successful_predictions:\n",
    "        # Calculate how many of each type to include\n",
    "        failed_sample_count = min(sample_count // 2, len(failed_predictions))\n",
    "        successful_sample_count = sample_count - failed_sample_count\n",
    "        \n",
    "        # Create a combined sample with both correct and incorrect predictions\n",
    "        selected_samples = (\n",
    "            random.sample(failed_predictions, failed_sample_count) + \n",
    "            random.sample(successful_predictions, min(successful_sample_count, len(successful_predictions)))\n",
    "        )\n",
    "    else:\n",
    "        # If one category is empty, just sample from all results\n",
    "        selected_samples = random.sample(evaluation_results, min(sample_count, len(evaluation_results)))\n",
    "    \n",
    "    # Build HTML visualization\n",
    "    visualization = '<h2>Character-level Error Visualization</h2>'\n",
    "    visualization += '<p>Green: Correct characters, Red: Incorrect characters</p>'\n",
    "    \n",
    "    # Process each selected sample\n",
    "    for index, sample in enumerate(selected_samples):\n",
    "        visualization += f'<h3>Sample {index + 1}</h3>'\n",
    "        \n",
    "        # Generate the character comparison HTML using the create_string_diff_visualization function\n",
    "        # (assuming the function was renamed as in the previous example)\n",
    "        visualization += create_string_diff_visualization(\n",
    "            sample['input'], \n",
    "            sample['prediction'], \n",
    "            sample['target']\n",
    "        )\n",
    "        \n",
    "        visualization += '<hr>'\n",
    "    \n",
    "    # Return HTML display object\n",
    "    return HTML(visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T12:57:08.357908Z",
     "iopub.status.busy": "2025-05-20T12:57:08.357650Z",
     "iopub.status.idle": "2025-05-20T12:57:08.474865Z",
     "shell.execute_reply": "2025-05-20T12:57:08.474324Z",
     "shell.execute_reply.started": "2025-05-20T12:57:08.357888Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sorting runs by -summary_metrics.validation_accuracy\n"
     ]
    }
   ],
   "source": [
    "api = wandb.Api()\n",
    "sweep = api.sweep('da24m014-iit-madras/DLA3/sweeps/4a34r0cv')\n",
    "best_run = sweep.best_run()\n",
    "best_run_config = best_run.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T12:57:09.672835Z",
     "iopub.status.busy": "2025-05-20T12:57:09.672164Z",
     "iopub.status.idle": "2025-05-20T12:57:09.678766Z",
     "shell.execute_reply": "2025-05-20T12:57:09.678040Z",
     "shell.execute_reply.started": "2025-05-20T12:57:09.672809Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 32,\n",
       " 'beam_width': 5,\n",
       " 'dropout_prob': 0.3,\n",
       " 'decoder_depth': 3,\n",
       " 'encoder_depth': 3,\n",
       " 'internal_size': 256,\n",
       " 'learning_rate': 0.001,\n",
       " 'embedding_size': 16,\n",
       " 'rnn_architecture': 'LSTM'}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_run_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T12:57:11.032409Z",
     "iopub.status.busy": "2025-05-20T12:57:11.031741Z",
     "iopub.status.idle": "2025-05-20T12:57:11.040250Z",
     "shell.execute_reply": "2025-05-20T12:57:11.039694Z",
     "shell.execute_reply.started": "2025-05-20T12:57:11.032386Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def test_and_evaluate(config=None):\n",
    "    run = wandb.init(config=config, project='DLA3')\n",
    "    cfg = run.config\n",
    "    epochs = 1\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    run.name = 'Testing without Attention'\n",
    "    model = TransliterationSystem(cfg, len(source_dict), len(target_dict))\n",
    "    model = model.to(device)\n",
    "    print('Model successfully moved to device.')\n",
    "    print('Loading datasets...')\n",
    "    train_dataset = TextConversionCorpus(train_file, source_dict, target_dict)\n",
    "    dev_dataset = TextConversionCorpus(dev_file, source_dict, target_dict)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True)\n",
    "    dev_loader = DataLoader(dev_dataset, batch_size=cfg.batch_size)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=cfg.learning_rate)\n",
    "    best_val_acc = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch {epoch + 1}/{epochs}')\n",
    "        train_loss, train_acc = training_iteration(model, train_loader, criterion, optimizer, device)\n",
    "        print(f'Train loss: {train_loss:.4f} Train Accuracy: {train_acc:.4f}')\n",
    "        val_loss, val_acc = validation_check(model, dev_loader, criterion, device)\n",
    "        print(f'Validation loss: {val_loss:.4f} Val Accuracy: {val_acc:.4f}')\n",
    "        wandb.log({'train_loss': train_loss, 'val_loss': val_loss, 'train_acc': train_acc, 'val_acc': val_acc, 'epoch': epoch})\n",
    "        if best_val_acc < val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'model.pth')\n",
    "    test_acc, results = evaluate_model(model, test_loader, source_dict, target_dict, device)\n",
    "    wandb.log({'Test_acc': test_acc})\n",
    "    display(visualize_errors(results, n_samples=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T12:57:13.031287Z",
     "iopub.status.busy": "2025-05-20T12:57:13.031031Z",
     "iopub.status.idle": "2025-05-20T12:59:41.206775Z",
     "shell.execute_reply": "2025-05-20T12:59:41.206066Z",
     "shell.execute_reply.started": "2025-05-20T12:57:13.031269Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized: LSTM, Encoder depth: 3, Decoder depth: 3, Embeddings: 16, Hidden size: 256\n",
      "Model successfully moved to device.\n",
      "Loading datasets...\n",
      "Successfully loaded 56303 entries from /kaggle/input/dakshina/dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.train.tsv\n",
      "Sample entries: [('angry', 'अँग्री'), ('aengeography', 'अँजिओग्राफी')]\n",
      "Successfully loaded 5658 entries from /kaggle/input/dakshina/dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.dev.tsv\n",
      "Sample entries: [('aendarsanla', 'अँडरसनला'), ('andersonla', 'अँडरसनला')]\n",
      "Epoch 1/1\n",
      "Train loss: 2.4234 Train Accuracy: 0.3341\n",
      "Validation loss: 1.5620 Val Accuracy: 0.5588\n",
      "Target vocabulary size: 69\n",
      "Special tokens: PAD=0, UNK=1, SOS=2, EOS=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5682/5682 [00:33<00:00, 168.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Word Accuracy: 0.0084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>Character-level Error Visualization</h2><p>Green: Correct characters, Red: Incorrect characters</p><h3>Sample 1</h3><p><b>Input:</b> kacharichya</p><p><b>Prediction vs Target:</b> <span style=\"color:green\">क</span><span style=\"color:red\">ा</span><span style=\"color:red\">ह</span><span style=\"color:red\">ा</span><span style=\"color:red\">र</span><span style=\"color:red\">ा</span><span style=\"color:red\">ं</span><span style=\"color:red\">च</span><span style=\"color:red\">ी</span></p><p><b>Target:</b> कचेरीच्या</p><hr><h3>Sample 2</h3><p><b>Input:</b> teri</p><p><b>Prediction vs Target:</b> <span style=\"color:green\">ट</span><span style=\"color:red\">ि</span><span style=\"color:green\">र</span><span style=\"color:green\">ी</span></p><p><b>Target:</b> टेरी</p><hr><h3>Sample 3</h3><p><b>Input:</b> mangdyaan</p><p><b>Prediction vs Target:</b> <span style=\"color:green\">म</span><span style=\"color:green\">ं</span><span style=\"color:green\">ग</span><span style=\"color:red\">ल</span><span style=\"color:red\">े</span><span style=\"color:red\">च</span><span style=\"color:red\">ा</span></p><p><b>Target:</b> मंगळयान</p><hr><h3>Sample 4</h3><p><b>Input:</b> rasiya</p><p><b>Prediction vs Target:</b> <span style=\"color:green\">र</span><span style=\"color:red\">ा</span><span style=\"color:red\">ज</span><span style=\"color:red\">ी</span><span style=\"color:red\">ल</span></p><p><b>Target:</b> रशिया</p><hr><h3>Sample 5</h3><p><b>Input:</b> parphaikt</p><p><b>Prediction vs Target:</b> <span style=\"color:green\">प</span><span style=\"color:green\">र</span><span style=\"color:red\">्</span><span style=\"color:red\">व</span><span style=\"color:red\">त</span><span style=\"color:red\">ी</span><span style=\"color:red\">त</span></p><p><b>Target:</b> परफेक्ट</p><hr><h3>Sample 6</h3><p><b>Input:</b> far</p><p><b>Prediction vs Target:</b> <span style=\"color:green\">फ</span><span style=\"color:green\">ा</span><span style=\"color:green\">र</span></p><p><b>Target:</b> फार</p><hr><h3>Sample 7</h3><p><b>Input:</b> naagaanchya</p><p><b>Prediction vs Target:</b> <span style=\"color:green\">न</span><span style=\"color:green\">ा</span><span style=\"color:green\">ग</span><span style=\"color:green\">ा</span><span style=\"color:green\">ं</span><span style=\"color:green\">च</span><span style=\"color:green\">्</span><span style=\"color:green\">य</span><span style=\"color:green\">ा</span></p><p><b>Target:</b> नागांच्या</p><hr><h3>Sample 8</h3><p><b>Input:</b> praant</p><p><b>Prediction vs Target:</b> <span style=\"color:green\">प</span><span style=\"color:green\">्</span><span style=\"color:green\">र</span><span style=\"color:green\">ा</span><span style=\"color:green\">ं</span><span style=\"color:green\">त</span></p><p><b>Target:</b> प्रांत</p><hr><h3>Sample 9</h3><p><b>Input:</b> joe</p><p><b>Prediction vs Target:</b> <span style=\"color:green\">ज</span><span style=\"color:green\">ो</span></p><p><b>Target:</b> जो</p><hr><h3>Sample 10</h3><p><b>Input:</b> potateel</p><p><b>Prediction vs Target:</b> <span style=\"color:green\">प</span><span style=\"color:green\">ो</span><span style=\"color:green\">ट</span><span style=\"color:green\">ा</span><span style=\"color:green\">त</span><span style=\"color:green\">ी</span><span style=\"color:green\">ल</span></p><p><b>Target:</b> पोटातील</p><hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_and_evaluate(best_run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7340302,
     "sourceId": 11694993,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
