{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11694993,"sourceType":"datasetVersion","datasetId":7340302}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Version 3","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport pandas as pd\nfrom torch.utils.data import DataLoader, Dataset\nimport wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T12:52:30.106485Z","iopub.execute_input":"2025-05-20T12:52:30.107022Z","iopub.status.idle":"2025-05-20T12:52:30.112580Z","shell.execute_reply.started":"2025-05-20T12:52:30.106996Z","shell.execute_reply":"2025-05-20T12:52:30.111813Z"}},"outputs":[],"execution_count":108},{"cell_type":"code","source":"class TextConversionCorpus(Dataset):\n    def __init__(self, data_file, source_dictionary=None, target_dictionary=None, sequence_limit=32, build_dictionary=False):\n        # Try to load the data file with romanized and native text\n        try:\n            df = pd.read_csv(data_file, sep='\\t', header=None,\n                          names=['original', 'transliterated', 'frequency'],\n                          usecols=[0, 1], dtype=str)\n            print(f\"Successfully loaded {len(df)} entries from {data_file}\")\n\n            # Handle missing values\n            df['original'] = df['original'].fillna('')\n            df['transliterated'] = df['transliterated'].fillna('')\n\n            # Create pairs for training\n            self.entry_pairs = list(zip(df['transliterated'], df['original']))\n            print(f\"Sample entries: {self.entry_pairs[:2]}\")\n        except Exception as e:\n            print(f\"Data loading error: {e}\")\n            self.entry_pairs = [('', '')]  # Default empty entry\n            \n        self.sequence_limit = sequence_limit\n        \n        # Setup dictionaries for conversion\n        if build_dictionary:\n            self.source_dictionary = {'<pad>': 0, '<unk>': 1, '<bos>': 2, '<eos>': 3}\n            self.target_dictionary = {'<pad>': 0, '<unk>': 1, '<bos>': 2, '<eos>': 3}\n            self._build_dictionaries()\n        else:\n            self.source_dictionary, self.target_dictionary = source_dictionary, target_dictionary\n            # Ensure special tokens exist\n            if '<eos>' not in self.source_dictionary:\n                self.source_dictionary['<eos>'] = len(self.source_dictionary)\n            if '<eos>' not in self.target_dictionary:\n                self.target_dictionary['<eos>'] = len(self.target_dictionary)\n    \n    def _build_dictionaries(self):\n        # Create character-level dictionaries from the dataset\n        for source_text, target_text in self.entry_pairs:\n            for character in source_text:\n                if character not in self.source_dictionary:\n                    self.source_dictionary[character] = len(self.source_dictionary)\n            for character in target_text:\n                if character not in self.target_dictionary:\n                    self.target_dictionary[character] = len(self.target_dictionary)\n        print(f\"Dictionary sizes — Source: {len(self.source_dictionary)}, Target: {len(self.target_dictionary)}\")\n    \n    def __len__(self):\n        return len(self.entry_pairs)\n    \n    def __getitem__(self, index):\n        source_text, target_text = self.entry_pairs[index]\n        \n        # Convert source text to indices\n        source_indices = [self.source_dictionary['<bos>']]  # Begin with start token\n        for char in source_text:\n            idx = self.source_dictionary.get(char, self.source_dictionary['<unk>'])\n            if idx >= len(self.source_dictionary):\n                idx = self.source_dictionary['<unk>']  # Safety check\n            source_indices.append(idx)\n        \n        # Convert target text to indices\n        target_indices = [self.target_dictionary['<bos>']]  # Begin with start token\n        for char in target_text:\n            idx = self.target_dictionary.get(char, self.target_dictionary['<unk>'])\n            if idx >= len(self.target_dictionary):\n                idx = self.target_dictionary['<unk>']  # Safety check\n            target_indices.append(idx)\n        \n        # Add end tokens\n        source_indices.append(self.source_dictionary['<eos>'])\n        target_indices.append(self.target_dictionary['<eos>'])\n        \n        # Add padding\n        src_padding = [self.source_dictionary['<pad>']] * max(0, self.sequence_limit - len(source_indices))\n        tgt_padding = [self.target_dictionary['<pad>']] * max(0, self.sequence_limit - len(target_indices))\n        \n        # Truncate if needed and convert to tensor\n        source_indices = (source_indices + src_padding)[:self.sequence_limit]\n        target_indices = (target_indices + tgt_padding)[:self.sequence_limit]\n        \n        # Validate padding index\n        assert self.source_dictionary['<pad>'] < len(self.source_dictionary), \"Source padding index out of range\"\n        assert self.target_dictionary['<pad>'] < len(self.target_dictionary), \"Target padding index out of range\"\n        \n        return torch.tensor(source_indices, dtype=torch.long), torch.tensor(target_indices, dtype=torch.long)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T12:52:32.104137Z","iopub.execute_input":"2025-05-20T12:52:32.104730Z","iopub.status.idle":"2025-05-20T12:52:32.117314Z","shell.execute_reply.started":"2025-05-20T12:52:32.104708Z","shell.execute_reply":"2025-05-20T12:52:32.116788Z"}},"outputs":[],"execution_count":109},{"cell_type":"code","source":"class SourceProcessor(nn.Module):\n    def __init__(self, vocab_size, embed_size, hidden_size, layers_count, dropout_rate=0.0, architecture_type='GRU'):\n        super().__init__()\n        self.char_embeddings = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n        self.hidden_size = hidden_size\n        self.layers_count = layers_count\n        self.architecture_type = architecture_type\n        \n        # Select the recurrent architecture\n        if architecture_type == 'SimpleRNN':\n            rnn_class = nn.RNN\n        elif architecture_type == 'GRU':\n            rnn_class = nn.GRU\n        elif architecture_type == 'LSTM':\n            rnn_class = nn.LSTM\n        else:\n            raise ValueError(f\"Unsupported architecture: {architecture_type}\")\n        \n        self.processor = rnn_class(\n            embed_size,\n            hidden_size,\n            num_layers=layers_count,\n            batch_first=True,\n            dropout=dropout_rate if layers_count > 1 else 0.0\n        )\n    \n    def forward(self, input_seq):\n        \"\"\"\n        Process input sequence through the encoder\n        \n        Args:\n            input_seq: Source sequence [batch_size, seq_length]\n        \n        Returns:\n            full_output: All states [batch_size, seq_length, hidden_size]\n            final_state: Final states [layers_count, batch_size, hidden_size]\n        \"\"\"\n        # Generate embeddings\n        embedded_chars = self.char_embeddings(input_seq)\n        \n        # Process through RNN\n        full_output, final_state = self.processor(embedded_chars)\n        \n        return full_output, final_state\n\n\nclass TargetGenerator(nn.Module):\n    def __init__(self, vocab_size, embed_size, hidden_size, layers_count, dropout_rate=0.0, architecture_type='GRU'):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.embed_size = embed_size\n        self.hidden_size = hidden_size\n        self.layers_count = layers_count\n        self.architecture_type = architecture_type\n        \n        self.char_embeddings = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n        \n        # Select the recurrent architecture\n        if architecture_type == 'SimpleRNN':\n            rnn_class = nn.RNN\n        elif architecture_type == 'GRU':\n            rnn_class = nn.GRU\n        elif architecture_type == 'LSTM':\n            rnn_class = nn.LSTM\n        else:\n            raise ValueError(f\"Unsupported architecture: {architecture_type}\")\n        \n        self.processor = rnn_class(\n            embed_size,\n            hidden_size,\n            num_layers=layers_count,\n            batch_first=True,\n            dropout=dropout_rate if layers_count > 1 else 0.0\n        )\n        \n        # Projection to vocabulary\n        self.output_mapper = nn.Linear(hidden_size, vocab_size)\n    \n    def forward(self, token_index, prev_state):\n        \"\"\"\n        Process a single decoder step\n        \n        Args:\n            token_index: Current token indices [batch_size, 1]\n            prev_state: Previous hidden state\n            \n        Returns:\n            output_probs: Output probabilities [batch_size, vocab_size]\n            new_state: Updated hidden state\n        \"\"\"\n        # Get embeddings for the current token\n        embedded_token = self.char_embeddings(token_index)\n        \n        # Process through RNN\n        output, new_state = self.processor(embedded_token, prev_state)\n        \n        # Project output to vocabulary size\n        output_probs = self.output_mapper(output.squeeze(1))\n        \n        return output_probs, new_state\n\n\nclass TransliterationSystem(nn.Module):\n    def __init__(self, params, input_vocab_size, output_vocab_size):\n        super().__init__()\n        \n        # Validate vocabulary sizes\n        assert input_vocab_size > 0, f\"Invalid input vocabulary size: {input_vocab_size}\"\n        assert output_vocab_size > 0, f\"Invalid output vocabulary size: {output_vocab_size}\"\n        \n        # Store configuration parameters\n        self.embedding_dim = params.embedding_size\n        self.internal_dim = params.internal_size\n        self.rnn_type = params.rnn_architecture\n        self.encoder_depth = params.encoder_depth\n        self.decoder_depth = params.decoder_depth\n        \n        # Initialize encoder and decoder components\n        self.encoder = SourceProcessor(\n            input_vocab_size,\n            params.embedding_size,\n            params.internal_size,\n            params.encoder_depth,\n            params.dropout_prob,\n            params.rnn_architecture\n        )\n        \n        self.decoder = TargetGenerator(\n            output_vocab_size,\n            params.embedding_size,\n            params.internal_size,\n            params.decoder_depth,\n            params.dropout_prob,\n            params.rnn_architecture\n        )\n        \n        # Store embeddings for access during training/inference\n        self.source_embeddings = self.encoder.char_embeddings\n        self.target_embeddings = self.decoder.char_embeddings\n        \n        print(f\"Model initialized: {self.rnn_type}, Encoder depth: {self.encoder_depth}, \"\n              f\"Decoder depth: {self.decoder_depth}, Embeddings: {self.embedding_dim}, \"\n              f\"Hidden size: {self.internal_dim}\")\n              \n    def forward(self, source_sequence, target_sequence):\n        \"\"\"\n        Forward pass through the complete transliteration model\n        \n        Args:\n            source_sequence: Source text indices [batch_size, src_len]\n            target_sequence: Target text indices [batch_size, tgt_len]\n            \n        Returns:\n            predictions: Output probabilities [batch_size, tgt_len-1, output_vocab_size]\n        \"\"\"\n        batch_size, src_len = source_sequence.size()\n        tgt_len = target_sequence.size(1)\n        device = source_sequence.device\n        \n        # Check for out-of-bounds indices and clamp if needed\n        if source_sequence.max() >= self.source_embeddings.num_embeddings:\n            print(\"Warning: Source indices out of vocabulary range\")\n            source_sequence = torch.clamp(source_sequence, 0, self.source_embeddings.num_embeddings - 1)\n        if target_sequence.max() >= self.target_embeddings.num_embeddings:\n            print(\"Warning: Target indices out of vocabulary range\")\n            target_sequence = torch.clamp(target_sequence, 0, self.target_embeddings.num_embeddings - 1)\n            \n        try:\n            # Process source sequence\n            _, encoder_state = self.encoder(source_sequence)\n            \n            # Handle mismatch in number of layers between encoder and decoder\n            if self.encoder_depth != self.decoder_depth:\n                # If encoder and decoder depths are different, need to adjust the hidden state\n                if isinstance(encoder_state, tuple):  # For LSTM (hidden state, cell state)\n                    # Adjust both hidden state and cell state\n                    h_state, c_state = encoder_state\n                    if self.encoder_depth > self.decoder_depth:\n                        # If encoder has more layers, take only what decoder needs\n                        decoder_h = h_state[-self.decoder_depth:]\n                        decoder_c = c_state[-self.decoder_depth:]\n                        decoder_state = (decoder_h, decoder_c)\n                    else:\n                        # If decoder has more layers, replicate encoder's last layer\n                        decoder_h = torch.cat([h_state, h_state[-1:].repeat(self.decoder_depth - self.encoder_depth, 1, 1)], 0)\n                        decoder_c = torch.cat([c_state, c_state[-1:].repeat(self.decoder_depth - self.encoder_depth, 1, 1)], 0)\n                        decoder_state = (decoder_h, decoder_c)\n                else:  # For GRU/RNN (just hidden state)\n                    if self.encoder_depth > self.decoder_depth:\n                        # If encoder has more layers, take only what decoder needs\n                        decoder_state = encoder_state[-self.decoder_depth:]\n                    else:\n                        # If decoder has more layers, replicate encoder's last layer\n                        decoder_state = torch.cat([encoder_state, \n                                                  encoder_state[-1:].repeat(self.decoder_depth - self.encoder_depth, 1, 1)], 0)\n            else:\n                # If depths match, use encoder state directly\n                decoder_state = encoder_state\n            \n            # Teacher forcing: use target tokens as inputs\n            decoder_inputs = target_sequence[:, :-1]  # From <bos> to second-to-last token\n            \n            # Create tensor for outputs\n            outputs = torch.zeros(batch_size, tgt_len-1, self.decoder.vocab_size, device=device)\n            \n            # Process each token sequentially\n            for t in range(tgt_len-1):\n                # Get current token\n                current_token = target_sequence[:, t].unsqueeze(1)\n                \n                # Process through decoder\n                decoder_output, decoder_state = self.decoder(\n                    current_token,\n                    decoder_state\n                )\n                \n                # Store prediction\n                outputs[:, t, :] = decoder_output\n                \n            return outputs\n            \n        except Exception as error:\n            print(f\"Error in forward pass: {error}\")\n            return torch.zeros(batch_size, tgt_len-1, self.decoder.vocab_size, device=device)\n            \n    def generate(self, source_sequence, max_length=50, beam_width=1):\n        \"\"\"\n        Generate transliteration using search methods\n        \n        Args:\n            source_sequence: Source sequence [batch_size, src_len]\n            max_length: Maximum generation length\n            beam_width: Width for beam search (1 = greedy)\n            \n        Returns:\n            generated_text: Generated sequence [batch_size, max_length]\n        \"\"\"\n        batch_size = source_sequence.size(0)\n        device = source_sequence.device\n        \n        # Encode source sequence\n        _, encoder_state = self.encoder(source_sequence)\n        \n        # Handle mismatch in number of layers between encoder and decoder\n        if self.encoder_depth != self.decoder_depth:\n            # If encoder and decoder depths are different, need to adjust the hidden state\n            if isinstance(encoder_state, tuple):  # For LSTM (hidden state, cell state)\n                # Adjust both hidden state and cell state\n                h_state, c_state = encoder_state\n                if self.encoder_depth > self.decoder_depth:\n                    # If encoder has more layers, take only what decoder needs\n                    decoder_h = h_state[-self.decoder_depth:]\n                    decoder_c = c_state[-self.decoder_depth:]\n                    decoder_state = (decoder_h, decoder_c)\n                else:\n                    # If decoder has more layers, replicate encoder's last layer\n                    decoder_h = torch.cat([h_state, h_state[-1:].repeat(self.decoder_depth - self.encoder_depth, 1, 1)], 0)\n                    decoder_c = torch.cat([c_state, c_state[-1:].repeat(self.decoder_depth - self.encoder_depth, 1, 1)], 0)\n                    decoder_state = (decoder_h, decoder_c)\n            else:  # For GRU/RNN (just hidden state)\n                if self.encoder_depth > self.decoder_depth:\n                    # If encoder has more layers, take only what decoder needs\n                    decoder_state = encoder_state[-self.decoder_depth:]\n                else:\n                    # If decoder has more layers, replicate encoder's last layer\n                    decoder_state = torch.cat([encoder_state, \n                                              encoder_state[-1:].repeat(self.decoder_depth - self.encoder_depth, 1, 1)], 0)\n        else:\n            # If depths match, use encoder state directly\n            decoder_state = encoder_state\n        \n        if beam_width == 1:\n            # Use greedy search\n            decoder_token = torch.tensor([[2]], device=device).repeat(batch_size, 1)  # <bos> token\n            output_sequence = torch.zeros(batch_size, max_length, dtype=torch.long, device=device)\n            \n            for t in range(max_length):\n                # Process through decoder\n                token_probs, decoder_state = self.decoder(\n                    decoder_token,\n                    decoder_state\n                )\n                \n                # Select most likely token\n                _, token_idx = token_probs.topk(1)\n                decoder_token = token_idx.view(batch_size, 1)\n                \n                # Store generated token\n                output_sequence[:, t] = decoder_token.squeeze(1)\n                \n                # Check for end-of-sequence\n                if (decoder_token == 3).all():  # 3 is <eos> token\n                    break\n                    \n            return output_sequence\n        else:\n            # Placeholder for beam search implementation\n            # Currently falls back to greedy search\n            return self.generate(source_sequence, max_length, beam_width=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T12:52:33.574601Z","iopub.execute_input":"2025-05-20T12:52:33.575160Z","iopub.status.idle":"2025-05-20T12:52:33.597744Z","shell.execute_reply.started":"2025-05-20T12:52:33.575140Z","shell.execute_reply":"2025-05-20T12:52:33.597126Z"}},"outputs":[],"execution_count":110},{"cell_type":"code","source":"def calculate_performance(logits, reference, pad_idx=0):\n    \"\"\"\n    Calculate character accuracy excluding padding tokens\n    \"\"\"\n    predictions = logits.argmax(dim=-1)\n    valid_positions = reference != pad_idx\n    correct_chars = (predictions == reference) & valid_positions\n    performance = correct_chars.sum().item() / max(valid_positions.sum().item(), 1)\n    return performance","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T12:52:39.476750Z","iopub.execute_input":"2025-05-20T12:52:39.476982Z","iopub.status.idle":"2025-05-20T12:52:39.482735Z","shell.execute_reply.started":"2025-05-20T12:52:39.476967Z","shell.execute_reply":"2025-05-20T12:52:39.481935Z"}},"outputs":[],"execution_count":111},{"cell_type":"code","source":"def training_iteration(model, dataloader, loss_function, optimizer, compute_device):\n    \"\"\"Run one training iteration\"\"\"\n    model.train()\n    total_loss = 0.0\n    total_performance = 0.0\n    total_batches = len(dataloader)\n    successful_batches = 0\n    \n    for batch_idx, (inputs, targets) in enumerate(dataloader):\n        try:\n            inputs = inputs.to(compute_device)\n            targets = targets.to(compute_device)\n            \n            # Check for vocabulary issues\n            if inputs.max().item() >= model.source_embeddings.num_embeddings or \\\n               targets.max().item() >= model.target_embeddings.num_embeddings:\n                print(f\"Skipping batch {batch_idx}/{total_batches} - vocabulary index issues detected\")\n                continue\n                \n            # Clear gradients\n            optimizer.zero_grad()\n            \n            # Forward pass\n            predictions = model(inputs, targets)\n            \n            # Reshape for loss calculation\n            flat_predictions = predictions.reshape(-1, predictions.size(-1))\n            flat_targets = targets[:, 1:].reshape(-1)  # Offset for teacher forcing\n            \n            # Calculate loss and backpropagate\n            batch_loss = loss_function(flat_predictions, flat_targets)\n            batch_loss.backward()\n            \n            # Apply gradient clipping\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            \n            # Update weights\n            optimizer.step()\n            \n            # Track metrics\n            total_loss += batch_loss.item()\n            \n            # Calculate accuracy\n            batch_accuracy = calculate_performance(predictions, targets[:, 1:])\n            total_performance += batch_accuracy\n            successful_batches += 1\n            \n        except Exception as error:\n            print(f\"Error processing batch {batch_idx}/{total_batches}: {error}\")\n            continue\n    \n    if successful_batches == 0:\n        return 0.0, 0.0\n    return total_loss / successful_batches, total_performance / successful_batches\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T12:52:40.831178Z","iopub.execute_input":"2025-05-20T12:52:40.831948Z","iopub.status.idle":"2025-05-20T12:52:40.840706Z","shell.execute_reply.started":"2025-05-20T12:52:40.831923Z","shell.execute_reply":"2025-05-20T12:52:40.839939Z"}},"outputs":[],"execution_count":112},{"cell_type":"code","source":"def validation_check(model, dataloader, loss_function, compute_device):\n    \"\"\"Evaluate model on validation data\"\"\"\n    model.eval()\n    total_loss = 0.0\n    total_performance = 0.0\n    successful_batches = 0\n    \n    with torch.no_grad():\n        for inputs, targets in dataloader:\n            try:\n                inputs = inputs.to(compute_device)\n                targets = targets.to(compute_device)\n                \n                # Check for vocabulary issues\n                if inputs.max() >= model.source_embeddings.num_embeddings or \\\n                   targets.max() >= model.target_embeddings.num_embeddings:\n                    continue\n                \n                # Forward pass\n                predictions = model(inputs, targets)\n                \n                # Reshape for loss calculation\n                flat_predictions = predictions.reshape(-1, predictions.size(-1))\n                flat_targets = targets[:, 1:].reshape(-1)  # Offset for teacher forcing\n                \n                # Calculate loss\n                batch_loss = loss_function(flat_predictions, flat_targets)\n                total_loss += batch_loss.item()\n                \n                # Calculate accuracy\n                batch_accuracy = calculate_performance(predictions, targets[:, 1:])\n                total_performance += batch_accuracy\n                successful_batches += 1\n                \n            except Exception as error:\n                print(f\"Evaluation error: {error}\")\n                continue\n    \n    if successful_batches == 0:\n        return 0.0, 0.0\n    return total_loss / successful_batches, total_performance / successful_batches\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T12:52:43.092911Z","iopub.execute_input":"2025-05-20T12:52:43.093161Z","iopub.status.idle":"2025-05-20T12:52:43.100920Z","shell.execute_reply.started":"2025-05-20T12:52:43.093143Z","shell.execute_reply":"2025-05-20T12:52:43.100346Z"}},"outputs":[],"execution_count":113},{"cell_type":"code","source":"hyperparameter_search = {\n    'method': 'bayes',\n    'metric': {'name': 'validation_accuracy', 'goal': 'maximize'},\n    'parameters': {\n        'embedding_size': {'values': [16, 32, 64, 256]},\n        'internal_size': {'values': [16, 32, 64, 256]},\n        'rnn_architecture': {'values': ['SimpleRNN', 'GRU', 'LSTM']},\n        'encoder_depth': {'values': [1, 2, 3]},\n        'decoder_depth': {'values': [1, 2, 3]},\n        'dropout_prob': {'values': [0.2, 0.3]},\n        'learning_rate': {'values': [1e-3, 1e-4]},\n        'batch_size': {'values': [32, 64]},\n        'beam_width': {'values': [1, 3, 5]}\n    }\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T12:52:43.847964Z","iopub.execute_input":"2025-05-20T12:52:43.848221Z","iopub.status.idle":"2025-05-20T12:52:43.854572Z","shell.execute_reply.started":"2025-05-20T12:52:43.848203Z","shell.execute_reply":"2025-05-20T12:52:43.853758Z"}},"outputs":[],"execution_count":114},{"cell_type":"code","source":"def save_dictionaries(output_dir, source_dict, target_dict):\n    \"\"\"Save source and target dictionaries to JSON files\"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    source_path = os.path.join(output_dir, 'source_dict.json')\n    target_path = os.path.join(output_dir, 'target_dict.json')\n    \n    with open(source_path, 'w', encoding='utf-8') as source_file:\n        json.dump(source_dict, source_file, indent=2, ensure_ascii=False)\n        \n    with open(target_path, 'w', encoding='utf-8') as target_file:\n        json.dump(target_dict, target_file, indent=2, ensure_ascii=False)\n\ndef load_dictionaries(directory):\n    \"\"\"Load source and target dictionaries from JSON files\"\"\"\n    with open(os.path.join(directory, 'source_dict.json'), 'r', encoding='utf-8') as source_file:\n        source_dict = json.load(source_file)\n        \n    with open(os.path.join(directory, 'target_dict.json'), 'r', encoding='utf-8') as target_file:\n        target_dict = json.load(target_file)\n        \n    return source_dict, target_dict\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T12:52:44.518397Z","iopub.execute_input":"2025-05-20T12:52:44.518700Z","iopub.status.idle":"2025-05-20T12:52:44.525510Z","shell.execute_reply.started":"2025-05-20T12:52:44.518682Z","shell.execute_reply":"2025-05-20T12:52:44.524936Z"}},"outputs":[],"execution_count":115},{"cell_type":"code","source":"train_file = '/kaggle/input/dakshina/dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.train.tsv'\ndev_file = '/kaggle/input/dakshina/dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.dev.tsv'\ndict_dir = '/kaggle/working/dictionaries'\n\nprint(\"Building dictionaries...\")\ntrain_corpus = TextConversionCorpus(train_file, build_dictionary=True)\nsource_dict, target_dict = train_corpus.source_dictionary, train_corpus.target_dictionary\n\nsave_dictionaries(dict_dir, source_dict, target_dict)\nprint(f\"Source dictionary: {len(source_dict)}, Target dictionary: {len(target_dict)}\")\n\n# Display dictionary samples\nprint(\"Source dictionary sample:\")\nfor char, idx in list(source_dict.items())[:10]:\n    print(f\"  {repr(char)} => {idx}\")\nprint(\"Target dictionary sample:\")\nfor char, idx in list(target_dict.items())[:10]:\n    print(f\"  {repr(char)} => {idx}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T12:52:46.311077Z","iopub.execute_input":"2025-05-20T12:52:46.311698Z","iopub.status.idle":"2025-05-20T12:52:46.486450Z","shell.execute_reply.started":"2025-05-20T12:52:46.311673Z","shell.execute_reply":"2025-05-20T12:52:46.485750Z"}},"outputs":[{"name":"stdout","text":"Building dictionaries...\nSuccessfully loaded 56303 entries from /kaggle/input/dakshina/dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.train.tsv\nSample entries: [('angry', 'अँग्री'), ('aengeography', 'अँजिओग्राफी')]\nDictionary sizes — Source: 30, Target: 69\nSource dictionary: 30, Target dictionary: 69\nSource dictionary sample:\n  '<pad>' => 0\n  '<unk>' => 1\n  '<bos>' => 2\n  '<eos>' => 3\n  'a' => 4\n  'n' => 5\n  'g' => 6\n  'r' => 7\n  'y' => 8\n  'e' => 9\nTarget dictionary sample:\n  '<pad>' => 0\n  '<unk>' => 1\n  '<bos>' => 2\n  '<eos>' => 3\n  'अ' => 4\n  'ँ' => 5\n  'ग' => 6\n  '्' => 7\n  'र' => 8\n  'ी' => 9\n","output_type":"stream"}],"execution_count":116},{"cell_type":"code","source":"def execute_hyperparameter_search():\n    \"\"\"Function to execute for each hyperparameter configuration\"\"\"\n    experiment = wandb.init()\n    config = experiment.config\n        \n    # Create descriptive experiment name\n    experiment.name = f\"{config.rnn_architecture}-e{config.embedding_size}-h{config.internal_size}-enc{config.encoder_depth}-dec{config.decoder_depth}-d{config.dropout_prob}-lr{config.learning_rate}-b{config.batch_size}-beam{config.beam_width}\"\n        \n    # Setup hardware\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    training_epochs = 20\n    \n    # Initialize the model\n    try:\n        # Create on CPU first\n        model = TransliterationSystem(config, len(source_dict), len(target_dict))\n        print(\"Model created, moving to target device...\")\n        # Move to target device\n        model = model.to(device)\n        print(f\"Model successfully loaded on {device}.\")\n    except Exception as e:\n        print(f\"Error initializing model on {device}: {e}\")\n        print(\"Falling back to CPU...\")\n        device = torch.device('cpu')\n        model = TransliterationSystem(config, len(source_dict), len(target_dict)).to(device)\n        \n    # Setup datasets\n    try:\n        print(\"Preparing datasets...\")\n        train_corpus = TextConversionCorpus(train_file, source_dict, target_dict)\n        dev_corpus = TextConversionCorpus(dev_file, source_dict, target_dict)\n            \n        # Create data loaders\n        train_loader = DataLoader(train_corpus, batch_size=config.batch_size, shuffle=True)\n        dev_loader = DataLoader(dev_corpus, batch_size=config.batch_size)\n    except Exception as e:\n        print(f\"Dataset preparation error: {e}\")\n        return\n        \n    # Setup training components\n    try:\n        loss_function = nn.CrossEntropyLoss(ignore_index=0)  # 0 is padding index\n        optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n    except Exception as e:\n        print(f\"Error setting up training components: {e}\")\n        return\n        \n    # Training loop\n    best_validation_accuracy = 0.0\n        \n    try:\n        for epoch in range(training_epochs):\n            print(f\"Epoch {epoch+1}/{training_epochs}\")\n                \n            # Train\n            train_loss, train_accuracy = training_iteration(model, train_loader, loss_function, optimizer, device)\n            print(f\"Training - Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}\")\n                \n            # Validate\n            val_loss, val_accuracy = validation_check(model, dev_loader, loss_function, device)\n            print(f\"Validation - Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}\")\n                \n            # Log metrics\n            wandb.log({\n                'training_loss': train_loss, \n                'validation_loss': val_loss, \n                'training_accuracy': train_accuracy, \n                'validation_accuracy': val_accuracy, \n                'epoch': epoch\n            })\n                \n            # Track best model\n            if val_accuracy > best_validation_accuracy:\n                best_validation_accuracy = val_accuracy\n                \n        # Log final best accuracy\n        wandb.log({'validation_accuracy': best_validation_accuracy})\n                    \n    except Exception as e:\n        print(f\"Training error: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T12:52:47.544400Z","iopub.execute_input":"2025-05-20T12:52:47.544945Z","iopub.status.idle":"2025-05-20T12:52:47.554735Z","shell.execute_reply.started":"2025-05-20T12:52:47.544922Z","shell.execute_reply":"2025-05-20T12:52:47.554162Z"}},"outputs":[],"execution_count":117},{"cell_type":"code","source":"wandb.login(key = \"d6f8c99f1fd73267470842bbf00f03ae845f7308\")\n# sweep_id = wandb.sweep(hyperparameter_search, project='DLA3')\n# wandb.agent(sweep_id, execute_hyperparameter_search, count=20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T12:52:51.010256Z","iopub.execute_input":"2025-05-20T12:52:51.010750Z","iopub.status.idle":"2025-05-20T12:52:51.017791Z","shell.execute_reply.started":"2025-05-20T12:52:51.010726Z","shell.execute_reply":"2025-05-20T12:52:51.017091Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"},{"execution_count":118,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":118},{"cell_type":"markdown","source":"Testing","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nfrom torch.utils.data import DataLoader\nimport os\nfrom tqdm import tqdm\nimport csv\nfrom IPython.display import display, HTML","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T12:52:54.572967Z","iopub.execute_input":"2025-05-20T12:52:54.573232Z","iopub.status.idle":"2025-05-20T12:52:54.579066Z","shell.execute_reply.started":"2025-05-20T12:52:54.573211Z","shell.execute_reply":"2025-05-20T12:52:54.578434Z"}},"outputs":[],"execution_count":119},{"cell_type":"code","source":"path_test = '/kaggle/input/dakshina/dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.test.tsv'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T12:52:55.925063Z","iopub.execute_input":"2025-05-20T12:52:55.925315Z","iopub.status.idle":"2025-05-20T12:52:55.930813Z","shell.execute_reply.started":"2025-05-20T12:52:55.925295Z","shell.execute_reply":"2025-05-20T12:52:55.930090Z"}},"outputs":[],"execution_count":120},{"cell_type":"code","source":"test_dataset = TextConversionCorpus(path_test, source_dict, target_dict)\ntest_loader = DataLoader(test_dataset, batch_size=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T12:52:57.366731Z","iopub.execute_input":"2025-05-20T12:52:57.366983Z","iopub.status.idle":"2025-05-20T12:52:57.385304Z","shell.execute_reply.started":"2025-05-20T12:52:57.366965Z","shell.execute_reply":"2025-05-20T12:52:57.384745Z"}},"outputs":[{"name":"stdout","text":"Successfully loaded 5682 entries from /kaggle/input/dakshina/dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.test.tsv\nSample entries: [('andarasan', 'अँडरसन'), ('andarasana', 'अँडरसन')]\n","output_type":"stream"}],"execution_count":121},{"cell_type":"code","source":"def inference_without_teacher_forcing(model, src, max_len=100, device=None):\n    \"\"\"\n    Generate sequence without teacher forcing\n    \n    Args:\n        model: The seq2seq model\n        src: Source sequence [batch_size, src_len]\n        max_len: Maximum length to generate\n        device: Device to run inference on\n        \n    Returns:\n        outputs: Generated sequence [batch_size, max_len]\n    \"\"\"\n    batch_size = src.size(0)\n    if device is None:\n        device = src.device\n    \n    # Initialize outputs tensor\n    outputs = torch.zeros(batch_size, max_len, dtype=torch.long, device=device)\n    \n    # Encode source sequence\n    _, encoder_state = model.encoder(src)\n    \n    # Handle mismatch in number of layers between encoder and decoder\n    if model.encoder_depth != model.decoder_depth:\n        # If encoder and decoder depths are different, need to adjust the hidden state\n        if isinstance(encoder_state, tuple):  # For LSTM (hidden state, cell state)\n            # Adjust both hidden state and cell state\n            h_state, c_state = encoder_state\n            if model.encoder_depth > model.decoder_depth:\n                # If encoder has more layers, take only what decoder needs\n                decoder_h = h_state[-model.decoder_depth:]\n                decoder_c = c_state[-model.decoder_depth:]\n                decoder_state = (decoder_h, decoder_c)\n            else:\n                # If decoder has more layers, replicate encoder's last layer\n                decoder_h = torch.cat([h_state, h_state[-1:].repeat(model.decoder_depth - model.encoder_depth, 1, 1)], 0)\n                decoder_c = torch.cat([c_state, c_state[-1:].repeat(model.decoder_depth - model.encoder_depth, 1, 1)], 0)\n                decoder_state = (decoder_h, decoder_c)\n        else:  # For GRU/RNN (just hidden state)\n            if model.encoder_depth > model.decoder_depth:\n                # If encoder has more layers, take only what decoder needs\n                decoder_state = encoder_state[-model.decoder_depth:]\n            else:\n                # If decoder has more layers, replicate encoder's last layer\n                decoder_state = torch.cat([encoder_state, \n                                          encoder_state[-1:].repeat(model.decoder_depth - model.encoder_depth, 1, 1)], 0)\n    else:\n        # If depths match, use encoder state directly\n        decoder_state = encoder_state\n    \n    # Start with <bos> token (assuming token ID 2 is BOS)\n    decoder_token = torch.full((batch_size, 1), 2, dtype=torch.long, device=device)\n    \n    for t in range(max_len):\n        # Process through decoder\n        # IMPORTANT: Ensure token_index has shape [batch_size, 1]\n        decoder_output, decoder_state = model.decoder(\n            decoder_token,\n            decoder_state\n        )\n        \n        # Get most likely token\n        _, topi = decoder_output.topk(1)\n        decoder_token = topi.view(batch_size, 1)  # Ensure shape is [batch_size, 1]\n        \n        # Save to outputs tensor\n        outputs[:, t] = decoder_token.squeeze(1)\n        \n        # Check if all sequences generated EOS token (assuming token ID 3 is EOS)\n        if (decoder_token == 3).all():\n            break\n    \n    return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T12:52:58.811131Z","iopub.execute_input":"2025-05-20T12:52:58.811790Z","iopub.status.idle":"2025-05-20T12:52:58.821318Z","shell.execute_reply.started":"2025-05-20T12:52:58.811769Z","shell.execute_reply":"2025-05-20T12:52:58.820569Z"}},"outputs":[],"execution_count":122},{"cell_type":"code","source":"def decode_seq(seq, char_vocab, end_token=3):\n    \"\"\"\n    Decode a sequence of token indices to characters\n    \n    Args:\n        seq: Sequence of indices\n        char_vocab: Character vocabulary (char -> idx mapping)\n        end_token: Token index representing end of sequence\n        \n    Returns:\n        Decoded string\n    \"\"\"\n    idx_to_char = {idx: ch for ch, idx in char_vocab.items()}\n    result = []\n    has_end_token = False\n    for idx in seq:\n        if idx == 0:  # Skip padding\n            continue\n        if idx == end_token:  # Stop at EOS\n            has_end_token = True\n            break\n        if idx in idx_to_char:\n            result.append(idx_to_char[idx])\n        else:\n            result.append('<UNK>')\n    decoded = ''.join(result)\n    return decoded\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T12:53:00.485686Z","iopub.execute_input":"2025-05-20T12:53:00.486121Z","iopub.status.idle":"2025-05-20T12:53:00.492343Z","shell.execute_reply.started":"2025-05-20T12:53:00.486099Z","shell.execute_reply":"2025-05-20T12:53:00.491710Z"}},"outputs":[],"execution_count":123},{"cell_type":"code","source":"def evaluate_model(model, test_loader, src_vocab, tgt_vocab, device, end_token=3, unk_token=1, output_file='predictions_vanilla.csv'):\n    \"\"\"\n    Evaluate model on test set and save results to CSV\n    \n    Args:\n        model: The trained model\n        test_loader: DataLoader for test dataset\n        src_vocab: Source vocabulary (char -> idx mapping)\n        tgt_vocab: Target vocabulary (char -> idx mapping)\n        device: Device to run evaluation on\n        end_token: Token index representing end of sequence (default=2)\n        unk_token: Token index representing unknown token (default=1)\n        output_file: Path to save CSV results\n        \n    Returns:\n        List of results with input, prediction, target, and correctness\n    \"\"\"\n    model.eval()\n    results = []\n    tgt_vocab_size = len(tgt_vocab)\n    idx_to_char = {idx: ch for ch, idx in tgt_vocab.items()}\n    print(f'Target vocabulary size: {len(tgt_vocab)}')\n    print(f'Special tokens: PAD={0}, UNK={unk_token}, SOS={2}, EOS={end_token}')\n    with torch.no_grad(), open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(['input', 'prediction', 'target', 'correct'])\n        for batch in tqdm(test_loader, desc='Evaluating'):\n            src, tgt = batch\n            src, tgt = (src.to(device), tgt.to(device))\n            if hasattr(model, 'inference'):\n                output = model.inference(src)\n            else:\n                output = inference_without_teacher_forcing(model, src, max_len=100, device=device)\n            for i in range(src.size(0)):\n                src_seq = src[i].cpu().numpy()\n                if tgt[i, 0].item() == unk_token:\n                    tgt_seq = tgt[i, 1:].cpu().numpy()\n                else:\n                    tgt_seq = tgt[i].cpu().numpy()\n                pred_seq = output[i].cpu().numpy()\n                src_str = decode_seq(src_seq, src_vocab, end_token)\n                tgt_str = decode_seq(tgt_seq, tgt_vocab, end_token)\n                pred_str = decode_seq(pred_seq, tgt_vocab, end_token)\n                correct = pred_str == tgt_str[5:]\n                writer.writerow([src_str[5:], pred_str, tgt_str[5:]])\n                results.append({'input': src_str[5:], 'prediction': pred_str, 'target': tgt_str[5:], 'correct': correct})\n    accuracy = sum((1 for r in results if r['correct'])) / len(results) if results else 0\n    print(f'Overall Word Accuracy: {accuracy:.4f}')\n    return (accuracy, results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T12:57:04.139968Z","iopub.execute_input":"2025-05-20T12:57:04.140616Z","iopub.status.idle":"2025-05-20T12:57:04.150157Z","shell.execute_reply.started":"2025-05-20T12:57:04.140593Z","shell.execute_reply":"2025-05-20T12:57:04.149507Z"}},"outputs":[],"execution_count":131},{"cell_type":"code","source":"def generate_char_comparison_html(input_str, pred_str, target_str):\n    \"\"\"\n    Generate HTML with character-by-character comparison\n    \"\"\"\n    html = f'<p><b>Input:</b> {input_str}</p>'\n    html += '<p><b>Prediction vs Target:</b> '\n    max_len = max(len(pred_str), len(target_str))\n    pred_str_padded = pred_str.ljust(max_len)\n    target_str_padded = target_str.ljust(max_len)\n    for p_char, t_char in zip(pred_str_padded, target_str_padded):\n        if p_char == t_char:\n            html += f'<span style=\"color:green\">{p_char}</span>'\n        else:\n            html += f'<span style=\"color:red\">{p_char}</span>'\n    html += '</p>'\n    html += f'<p><b>Target:</b> {target_str}</p>'\n    return html","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T12:57:05.729591Z","iopub.execute_input":"2025-05-20T12:57:05.729831Z","iopub.status.idle":"2025-05-20T12:57:05.736056Z","shell.execute_reply.started":"2025-05-20T12:57:05.729814Z","shell.execute_reply":"2025-05-20T12:57:05.735424Z"}},"outputs":[],"execution_count":132},{"cell_type":"code","source":"def visualize_errors(results, n_samples=10):\n    \"\"\"\n    Visualize character-level errors for random samples\n    \"\"\"\n    incorrect_samples = [r for r in results if not r['correct']]\n    correct_samples = [r for r in results if r['correct']]\n    if len(incorrect_samples) > 0 and len(correct_samples) > 0:\n        n_incorrect = min(n_samples // 2, len(incorrect_samples))\n        n_correct = n_samples - n_incorrect\n        samples = random.sample(incorrect_samples, n_incorrect) + random.sample(correct_samples, min(n_correct, len(correct_samples)))\n    else:\n        samples = random.sample(results, min(n_samples, len(results)))\n    html_output = '<h2>Character-level Error Visualization</h2>'\n    html_output += '<p>Green: Correct characters, Red: Incorrect characters</p>'\n    for i, sample in enumerate(samples):\n        html_output += f'<h3>Sample {i + 1}</h3>'\n        html_output += generate_char_comparison_html(sample['input'], sample['prediction'], sample['target'])\n        html_output += '<hr>'\n    return HTML(html_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T12:57:07.058293Z","iopub.execute_input":"2025-05-20T12:57:07.058567Z","iopub.status.idle":"2025-05-20T12:57:07.065542Z","shell.execute_reply.started":"2025-05-20T12:57:07.058531Z","shell.execute_reply":"2025-05-20T12:57:07.064848Z"}},"outputs":[],"execution_count":133},{"cell_type":"code","source":"api = wandb.Api()\nsweep = api.sweep('da24m014-iit-madras/DLA3/sweeps/4a34r0cv')\nbest_run = sweep.best_run()\nbest_run_config = best_run.config","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T12:57:08.357650Z","iopub.execute_input":"2025-05-20T12:57:08.357908Z","iopub.status.idle":"2025-05-20T12:57:08.474865Z","shell.execute_reply.started":"2025-05-20T12:57:08.357888Z","shell.execute_reply":"2025-05-20T12:57:08.474324Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Sorting runs by -summary_metrics.validation_accuracy\n","output_type":"stream"}],"execution_count":134},{"cell_type":"code","source":"best_run_config","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T12:57:09.672164Z","iopub.execute_input":"2025-05-20T12:57:09.672835Z","iopub.status.idle":"2025-05-20T12:57:09.678766Z","shell.execute_reply.started":"2025-05-20T12:57:09.672809Z","shell.execute_reply":"2025-05-20T12:57:09.678040Z"}},"outputs":[{"execution_count":135,"output_type":"execute_result","data":{"text/plain":"{'batch_size': 32,\n 'beam_width': 5,\n 'dropout_prob': 0.3,\n 'decoder_depth': 3,\n 'encoder_depth': 3,\n 'internal_size': 256,\n 'learning_rate': 0.001,\n 'embedding_size': 16,\n 'rnn_architecture': 'LSTM'}"},"metadata":{}}],"execution_count":135},{"cell_type":"code","source":"def test_and_evaluate(config=None):\n    run = wandb.init(config=config, project='DLA3')\n    cfg = run.config\n    epochs = 1\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    run.name = 'Testing without Attention'\n    model = TransliterationSystem(cfg, len(source_dict), len(target_dict))\n    model = model.to(device)\n    print('Model successfully moved to device.')\n    print('Loading datasets...')\n    train_dataset = TextConversionCorpus(train_file, source_dict, target_dict)\n    dev_dataset = TextConversionCorpus(dev_file, source_dict, target_dict)\n    train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True)\n    dev_loader = DataLoader(dev_dataset, batch_size=cfg.batch_size)\n    criterion = nn.CrossEntropyLoss(ignore_index=0)\n    optimizer = optim.Adam(model.parameters(), lr=cfg.learning_rate)\n    best_val_acc = 0.0\n    for epoch in range(epochs):\n        print(f'Epoch {epoch + 1}/{epochs}')\n        train_loss, train_acc = training_iteration(model, train_loader, criterion, optimizer, device)\n        print(f'Train loss: {train_loss:.4f} Train Accuracy: {train_acc:.4f}')\n        val_loss, val_acc = validation_check(model, dev_loader, criterion, device)\n        print(f'Validation loss: {val_loss:.4f} Val Accuracy: {val_acc:.4f}')\n        wandb.log({'train_loss': train_loss, 'val_loss': val_loss, 'train_acc': train_acc, 'val_acc': val_acc, 'epoch': epoch})\n        if best_val_acc < val_acc:\n            best_val_acc = val_acc\n            torch.save(model.state_dict(), 'model.pth')\n    test_acc, results = evaluate_model(model, test_loader, source_dict, target_dict, device)\n    wandb.log({'Test_acc': test_acc})\n    display(visualize_errors(results, n_samples=10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T12:57:11.031741Z","iopub.execute_input":"2025-05-20T12:57:11.032409Z","iopub.status.idle":"2025-05-20T12:57:11.040250Z","shell.execute_reply.started":"2025-05-20T12:57:11.032386Z","shell.execute_reply":"2025-05-20T12:57:11.039694Z"}},"outputs":[],"execution_count":136},{"cell_type":"code","source":"test_and_evaluate(best_run_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T12:57:13.031031Z","iopub.execute_input":"2025-05-20T12:57:13.031287Z","iopub.status.idle":"2025-05-20T12:59:41.206775Z","shell.execute_reply.started":"2025-05-20T12:57:13.031269Z","shell.execute_reply":"2025-05-20T12:59:41.206066Z"}},"outputs":[{"name":"stdout","text":"Model initialized: LSTM, Encoder depth: 3, Decoder depth: 3, Embeddings: 16, Hidden size: 256\nModel successfully moved to device.\nLoading datasets...\nSuccessfully loaded 56303 entries from /kaggle/input/dakshina/dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.train.tsv\nSample entries: [('angry', 'अँग्री'), ('aengeography', 'अँजिओग्राफी')]\nSuccessfully loaded 5658 entries from /kaggle/input/dakshina/dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.dev.tsv\nSample entries: [('aendarsanla', 'अँडरसनला'), ('andersonla', 'अँडरसनला')]\nEpoch 1/1\nTrain loss: 2.4234 Train Accuracy: 0.3341\nValidation loss: 1.5620 Val Accuracy: 0.5588\nTarget vocabulary size: 69\nSpecial tokens: PAD=0, UNK=1, SOS=2, EOS=3\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 5682/5682 [00:33<00:00, 168.60it/s]","output_type":"stream"},{"name":"stdout","text":"Overall Word Accuracy: 0.0084\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<h2>Character-level Error Visualization</h2><p>Green: Correct characters, Red: Incorrect characters</p><h3>Sample 1</h3><p><b>Input:</b> kacharichya</p><p><b>Prediction vs Target:</b> <span style=\"color:green\">क</span><span style=\"color:red\">ा</span><span style=\"color:red\">ह</span><span style=\"color:red\">ा</span><span style=\"color:red\">र</span><span style=\"color:red\">ा</span><span style=\"color:red\">ं</span><span style=\"color:red\">च</span><span style=\"color:red\">ी</span></p><p><b>Target:</b> कचेरीच्या</p><hr><h3>Sample 2</h3><p><b>Input:</b> teri</p><p><b>Prediction vs Target:</b> <span style=\"color:green\">ट</span><span style=\"color:red\">ि</span><span style=\"color:green\">र</span><span style=\"color:green\">ी</span></p><p><b>Target:</b> टेरी</p><hr><h3>Sample 3</h3><p><b>Input:</b> mangdyaan</p><p><b>Prediction vs Target:</b> <span style=\"color:green\">म</span><span style=\"color:green\">ं</span><span style=\"color:green\">ग</span><span style=\"color:red\">ल</span><span style=\"color:red\">े</span><span style=\"color:red\">च</span><span style=\"color:red\">ा</span></p><p><b>Target:</b> मंगळयान</p><hr><h3>Sample 4</h3><p><b>Input:</b> rasiya</p><p><b>Prediction vs Target:</b> <span style=\"color:green\">र</span><span style=\"color:red\">ा</span><span style=\"color:red\">ज</span><span style=\"color:red\">ी</span><span style=\"color:red\">ल</span></p><p><b>Target:</b> रशिया</p><hr><h3>Sample 5</h3><p><b>Input:</b> parphaikt</p><p><b>Prediction vs Target:</b> <span style=\"color:green\">प</span><span style=\"color:green\">र</span><span style=\"color:red\">्</span><span style=\"color:red\">व</span><span style=\"color:red\">त</span><span style=\"color:red\">ी</span><span style=\"color:red\">त</span></p><p><b>Target:</b> परफेक्ट</p><hr><h3>Sample 6</h3><p><b>Input:</b> far</p><p><b>Prediction vs Target:</b> <span style=\"color:green\">फ</span><span style=\"color:green\">ा</span><span style=\"color:green\">र</span></p><p><b>Target:</b> फार</p><hr><h3>Sample 7</h3><p><b>Input:</b> naagaanchya</p><p><b>Prediction vs Target:</b> <span style=\"color:green\">न</span><span style=\"color:green\">ा</span><span style=\"color:green\">ग</span><span style=\"color:green\">ा</span><span style=\"color:green\">ं</span><span style=\"color:green\">च</span><span style=\"color:green\">्</span><span style=\"color:green\">य</span><span style=\"color:green\">ा</span></p><p><b>Target:</b> नागांच्या</p><hr><h3>Sample 8</h3><p><b>Input:</b> praant</p><p><b>Prediction vs Target:</b> <span style=\"color:green\">प</span><span style=\"color:green\">्</span><span style=\"color:green\">र</span><span style=\"color:green\">ा</span><span style=\"color:green\">ं</span><span style=\"color:green\">त</span></p><p><b>Target:</b> प्रांत</p><hr><h3>Sample 9</h3><p><b>Input:</b> joe</p><p><b>Prediction vs Target:</b> <span style=\"color:green\">ज</span><span style=\"color:green\">ो</span></p><p><b>Target:</b> जो</p><hr><h3>Sample 10</h3><p><b>Input:</b> potateel</p><p><b>Prediction vs Target:</b> <span style=\"color:green\">प</span><span style=\"color:green\">ो</span><span style=\"color:green\">ट</span><span style=\"color:green\">ा</span><span style=\"color:green\">त</span><span style=\"color:green\">ी</span><span style=\"color:green\">ल</span></p><p><b>Target:</b> पोटातील</p><hr>"},"metadata":{}}],"execution_count":137},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}